{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(517, 14) (517,)\n",
      "loss = 115.977723\n",
      "accuracy = 0.678917\n",
      "loss = 110.411866\n",
      "accuracy = 0.702128\n",
      "loss = 105.440735\n",
      "accuracy = 0.702128\n",
      "loss = 100.856699\n",
      "accuracy = 0.725338\n",
      "loss = 96.618101\n",
      "accuracy = 0.736944\n",
      "loss = 92.919840\n",
      "accuracy = 0.742747\n",
      "loss = 89.482414\n",
      "accuracy = 0.748549\n",
      "loss = 86.275562\n",
      "accuracy = 0.756286\n",
      "loss = 83.287140\n",
      "accuracy = 0.752418\n",
      "loss = 80.492079\n",
      "accuracy = 0.756286\n",
      "loss = 78.029662\n",
      "accuracy = 0.756286\n",
      "loss = 75.778074\n",
      "accuracy = 0.760155\n",
      "loss = 73.718000\n",
      "accuracy = 0.765957\n",
      "loss = 71.912060\n",
      "accuracy = 0.767892\n",
      "loss = 70.334309\n",
      "accuracy = 0.781431\n",
      "loss = 68.855497\n",
      "accuracy = 0.787234\n",
      "loss = 67.460333\n",
      "accuracy = 0.787234\n",
      "loss = 66.170778\n",
      "accuracy = 0.793037\n",
      "loss = 64.966055\n",
      "accuracy = 0.800774\n",
      "loss = 63.869357\n",
      "accuracy = 0.806576\n",
      "loss = 62.833260\n",
      "accuracy = 0.812379\n",
      "loss = 61.830992\n",
      "accuracy = 0.816248\n",
      "loss = 60.859442\n",
      "accuracy = 0.823985\n",
      "loss = 59.973043\n",
      "accuracy = 0.831721\n",
      "loss = 59.121179\n",
      "accuracy = 0.833656\n",
      "loss = 58.283742\n",
      "accuracy = 0.835590\n",
      "loss = 57.505557\n",
      "accuracy = 0.835590\n",
      "loss = 56.790346\n",
      "accuracy = 0.839458\n",
      "loss = 56.108599\n",
      "accuracy = 0.841393\n",
      "loss = 55.492850\n",
      "accuracy = 0.841393\n",
      "loss = 54.908936\n",
      "accuracy = 0.845261\n",
      "loss = 54.364404\n",
      "accuracy = 0.847195\n",
      "loss = 53.860291\n",
      "accuracy = 0.843327\n",
      "loss = 53.387329\n",
      "accuracy = 0.845261\n",
      "loss = 52.962463\n",
      "accuracy = 0.845261\n",
      "loss = 52.569793\n",
      "accuracy = 0.845261\n",
      "loss = 52.207318\n",
      "accuracy = 0.847195\n",
      "loss = 51.895018\n",
      "accuracy = 0.843327\n",
      "loss = 51.591593\n",
      "accuracy = 0.845261\n",
      "loss = 51.296421\n",
      "accuracy = 0.847195\n",
      "loss = 51.032398\n",
      "accuracy = 0.847195\n",
      "loss = 50.772633\n",
      "accuracy = 0.847195\n",
      "loss = 50.530967\n",
      "accuracy = 0.847195\n",
      "loss = 50.315296\n",
      "accuracy = 0.849130\n",
      "loss = 50.121779\n",
      "accuracy = 0.849130\n",
      "loss = 49.937124\n",
      "accuracy = 0.851064\n",
      "loss = 49.761581\n",
      "accuracy = 0.851064\n",
      "loss = 49.593461\n",
      "accuracy = 0.851064\n",
      "loss = 49.425543\n",
      "accuracy = 0.851064\n",
      "loss = 49.260216\n",
      "accuracy = 0.851064\n",
      "loss = 49.101266\n",
      "accuracy = 0.851064\n",
      "loss = 48.950865\n",
      "accuracy = 0.849130\n",
      "loss = 48.808234\n",
      "accuracy = 0.849130\n",
      "loss = 48.667829\n",
      "accuracy = 0.851064\n",
      "loss = 48.537667\n",
      "accuracy = 0.852998\n",
      "loss = 48.421788\n",
      "accuracy = 0.852998\n",
      "loss = 48.313775\n",
      "accuracy = 0.854932\n",
      "loss = 48.211916\n",
      "accuracy = 0.856867\n",
      "loss = 48.114158\n",
      "accuracy = 0.856867\n",
      "loss = 48.017888\n",
      "accuracy = 0.856867\n",
      "loss = 47.929685\n",
      "accuracy = 0.856867\n",
      "loss = 47.838750\n",
      "accuracy = 0.858801\n",
      "loss = 47.747908\n",
      "accuracy = 0.856867\n",
      "loss = 47.659484\n",
      "accuracy = 0.856867\n",
      "loss = 47.577998\n",
      "accuracy = 0.856867\n",
      "loss = 47.497860\n",
      "accuracy = 0.856867\n",
      "loss = 47.418006\n",
      "accuracy = 0.854932\n",
      "loss = 47.341902\n",
      "accuracy = 0.854932\n",
      "loss = 47.273400\n",
      "accuracy = 0.856867\n",
      "loss = 47.206684\n",
      "accuracy = 0.856867\n",
      "loss = 47.148457\n",
      "accuracy = 0.856867\n",
      "loss = 47.083448\n",
      "accuracy = 0.856867\n",
      "loss = 47.016973\n",
      "accuracy = 0.856867\n",
      "loss = 46.955814\n",
      "accuracy = 0.856867\n",
      "loss = 46.890370\n",
      "accuracy = 0.856867\n",
      "loss = 46.829006\n",
      "accuracy = 0.856867\n",
      "loss = 46.773162\n",
      "accuracy = 0.856867\n",
      "loss = 46.716108\n",
      "accuracy = 0.856867\n",
      "loss = 46.662552\n",
      "accuracy = 0.854932\n",
      "loss = 46.607442\n",
      "accuracy = 0.854932\n",
      "loss = 46.552377\n",
      "accuracy = 0.854932\n",
      "loss = 46.498474\n",
      "accuracy = 0.854932\n",
      "loss = 46.443438\n",
      "accuracy = 0.854932\n",
      "loss = 46.388698\n",
      "accuracy = 0.854932\n",
      "loss = 46.335341\n",
      "accuracy = 0.854932\n",
      "loss = 46.284453\n",
      "accuracy = 0.854932\n",
      "loss = 46.230414\n",
      "accuracy = 0.854932\n",
      "loss = 46.177942\n",
      "accuracy = 0.854932\n",
      "loss = 46.126987\n",
      "accuracy = 0.854932\n",
      "loss = 46.076615\n",
      "accuracy = 0.852998\n",
      "loss = 46.031717\n",
      "accuracy = 0.852998\n",
      "loss = 45.981079\n",
      "accuracy = 0.852998\n",
      "loss = 45.933805\n",
      "accuracy = 0.852998\n",
      "loss = 45.888663\n",
      "accuracy = 0.852998\n",
      "loss = 45.845223\n",
      "accuracy = 0.852998\n",
      "loss = 45.798787\n",
      "accuracy = 0.852998\n",
      "loss = 45.753561\n",
      "accuracy = 0.852998\n",
      "loss = 45.708334\n",
      "accuracy = 0.852998\n",
      "loss = 45.663108\n",
      "accuracy = 0.851064\n",
      "loss = 45.617905\n",
      "accuracy = 0.851064\n",
      "loss = 45.572242\n",
      "accuracy = 0.851064\n",
      "loss = 45.527662\n",
      "accuracy = 0.851064\n",
      "loss = 45.478240\n",
      "accuracy = 0.851064\n",
      "loss = 45.435416\n",
      "accuracy = 0.851064\n",
      "loss = 45.382777\n",
      "accuracy = 0.851064\n",
      "loss = 45.338749\n",
      "accuracy = 0.851064\n",
      "loss = 45.286413\n",
      "accuracy = 0.851064\n",
      "loss = 45.242168\n",
      "accuracy = 0.851064\n",
      "loss = 45.194972\n",
      "accuracy = 0.851064\n",
      "loss = 45.155648\n",
      "accuracy = 0.851064\n",
      "loss = 45.107831\n",
      "accuracy = 0.851064\n",
      "loss = 45.063778\n",
      "accuracy = 0.851064\n",
      "loss = 45.022226\n",
      "accuracy = 0.851064\n",
      "loss = 44.978103\n",
      "accuracy = 0.851064\n",
      "loss = 44.936603\n",
      "accuracy = 0.851064\n",
      "loss = 44.892465\n",
      "accuracy = 0.851064\n",
      "loss = 44.851411\n",
      "accuracy = 0.851064\n",
      "loss = 44.816292\n",
      "accuracy = 0.851064\n",
      "loss = 44.768389\n",
      "accuracy = 0.851064\n",
      "loss = 44.734242\n",
      "accuracy = 0.851064\n",
      "loss = 44.693982\n",
      "accuracy = 0.851064\n",
      "loss = 44.658138\n",
      "accuracy = 0.851064\n",
      "loss = 44.620034\n",
      "accuracy = 0.851064\n",
      "loss = 44.577545\n",
      "accuracy = 0.851064\n",
      "loss = 44.549277\n",
      "accuracy = 0.851064\n",
      "loss = 44.506589\n",
      "accuracy = 0.851064\n",
      "loss = 44.468300\n",
      "accuracy = 0.851064\n",
      "loss = 44.430032\n",
      "accuracy = 0.851064\n",
      "loss = 44.394730\n",
      "accuracy = 0.851064\n",
      "loss = 44.351742\n",
      "accuracy = 0.851064\n",
      "loss = 44.321715\n",
      "accuracy = 0.851064\n",
      "loss = 44.280324\n",
      "accuracy = 0.851064\n",
      "loss = 44.238103\n",
      "accuracy = 0.851064\n",
      "loss = 44.210537\n",
      "accuracy = 0.851064\n",
      "loss = 44.164316\n",
      "accuracy = 0.851064\n",
      "loss = 44.129134\n",
      "accuracy = 0.851064\n",
      "loss = 44.094304\n",
      "accuracy = 0.851064\n",
      "loss = 44.057124\n",
      "accuracy = 0.851064\n",
      "loss = 44.022509\n",
      "accuracy = 0.851064\n",
      "loss = 43.988545\n",
      "accuracy = 0.851064\n",
      "loss = 43.951679\n",
      "accuracy = 0.851064\n",
      "loss = 43.915283\n",
      "accuracy = 0.851064\n",
      "loss = 43.883709\n",
      "accuracy = 0.851064\n",
      "loss = 43.844897\n",
      "accuracy = 0.851064\n",
      "loss = 43.815255\n",
      "accuracy = 0.851064\n",
      "loss = 43.772639\n",
      "accuracy = 0.851064\n",
      "loss = 43.741086\n",
      "accuracy = 0.851064\n",
      "loss = 43.707420\n",
      "accuracy = 0.851064\n",
      "loss = 43.677393\n",
      "accuracy = 0.851064\n",
      "loss = 43.638332\n",
      "accuracy = 0.851064\n",
      "loss = 43.611990\n",
      "accuracy = 0.851064\n",
      "loss = 43.579318\n",
      "accuracy = 0.851064\n",
      "loss = 43.549198\n",
      "accuracy = 0.851064\n",
      "loss = 43.516627\n",
      "accuracy = 0.851064\n",
      "loss = 43.486558\n",
      "accuracy = 0.851064\n",
      "loss = 43.451452\n",
      "accuracy = 0.851064\n",
      "loss = 43.421317\n",
      "accuracy = 0.851064\n",
      "loss = 43.385956\n",
      "accuracy = 0.851064\n",
      "loss = 43.353816\n",
      "accuracy = 0.851064\n",
      "loss = 43.321100\n",
      "accuracy = 0.851064\n",
      "loss = 43.291871\n",
      "accuracy = 0.851064\n",
      "loss = 43.257099\n",
      "accuracy = 0.851064\n",
      "loss = 43.232976\n",
      "accuracy = 0.851064\n",
      "loss = 43.199962\n",
      "accuracy = 0.851064\n",
      "loss = 43.177119\n",
      "accuracy = 0.851064\n",
      "loss = 43.142878\n",
      "accuracy = 0.851064\n",
      "loss = 43.120454\n",
      "accuracy = 0.851064\n",
      "loss = 43.094543\n",
      "accuracy = 0.851064\n",
      "loss = 43.070289\n",
      "accuracy = 0.851064\n",
      "loss = 43.043340\n",
      "accuracy = 0.851064\n",
      "loss = 43.020985\n",
      "accuracy = 0.851064\n",
      "loss = 42.986330\n",
      "accuracy = 0.851064\n",
      "loss = 42.966096\n",
      "accuracy = 0.851064\n",
      "loss = 42.943696\n",
      "accuracy = 0.851064\n",
      "loss = 42.910524\n",
      "accuracy = 0.851064\n",
      "loss = 42.890216\n",
      "accuracy = 0.851064\n",
      "loss = 42.862950\n",
      "accuracy = 0.851064\n",
      "loss = 42.844797\n",
      "accuracy = 0.851064\n",
      "loss = 42.809926\n",
      "accuracy = 0.851064\n",
      "loss = 42.792453\n",
      "accuracy = 0.851064\n",
      "loss = 42.760557\n",
      "accuracy = 0.851064\n",
      "loss = 42.745819\n",
      "accuracy = 0.851064\n",
      "loss = 42.712633\n",
      "accuracy = 0.851064\n",
      "loss = 42.704700\n",
      "accuracy = 0.851064\n",
      "loss = 42.670824\n",
      "accuracy = 0.851064\n",
      "loss = 42.663636\n",
      "accuracy = 0.851064\n",
      "loss = 42.632479\n",
      "accuracy = 0.851064\n",
      "loss = 42.617820\n",
      "accuracy = 0.851064\n",
      "loss = 42.593210\n",
      "accuracy = 0.851064\n",
      "loss = 42.576211\n",
      "accuracy = 0.851064\n",
      "loss = 42.548042\n",
      "accuracy = 0.851064\n",
      "loss = 42.538964\n",
      "accuracy = 0.851064\n",
      "loss = 42.509415\n",
      "accuracy = 0.851064\n",
      "loss = 42.497196\n",
      "accuracy = 0.851064\n",
      "loss = 42.469955\n",
      "accuracy = 0.851064\n",
      "loss = 42.455321\n",
      "accuracy = 0.851064\n",
      "loss = 42.431016\n",
      "accuracy = 0.851064\n",
      "loss = 42.414518\n",
      "accuracy = 0.851064\n",
      "loss = 42.392841\n",
      "accuracy = 0.851064\n",
      "loss = 42.376536\n",
      "accuracy = 0.851064\n",
      "loss = 42.353042\n",
      "accuracy = 0.851064\n",
      "loss = 42.338278\n",
      "accuracy = 0.851064\n",
      "loss = 42.318496\n",
      "accuracy = 0.851064\n",
      "loss = 42.303024\n",
      "accuracy = 0.851064\n",
      "loss = 42.285234\n",
      "accuracy = 0.851064\n",
      "loss = 42.271539\n",
      "accuracy = 0.851064\n",
      "loss = 42.250876\n",
      "accuracy = 0.851064\n",
      "loss = 42.237260\n",
      "accuracy = 0.851064\n",
      "loss = 42.219939\n",
      "accuracy = 0.851064\n",
      "loss = 42.207102\n",
      "accuracy = 0.851064\n",
      "loss = 42.190558\n",
      "accuracy = 0.851064\n",
      "loss = 42.172743\n",
      "accuracy = 0.851064\n",
      "loss = 42.157316\n",
      "accuracy = 0.851064\n",
      "loss = 42.138176\n",
      "accuracy = 0.851064\n",
      "loss = 42.123123\n",
      "accuracy = 0.851064\n",
      "loss = 42.104656\n",
      "accuracy = 0.851064\n",
      "loss = 42.090064\n",
      "accuracy = 0.851064\n",
      "loss = 42.072522\n",
      "accuracy = 0.851064\n",
      "loss = 42.058135\n",
      "accuracy = 0.851064\n",
      "loss = 42.043112\n",
      "accuracy = 0.851064\n",
      "loss = 42.031502\n",
      "accuracy = 0.851064\n",
      "loss = 42.014092\n",
      "accuracy = 0.851064\n",
      "loss = 42.003272\n",
      "accuracy = 0.851064\n",
      "loss = 41.989048\n",
      "accuracy = 0.851064\n",
      "loss = 41.988128\n",
      "accuracy = 0.851064\n",
      "loss = 41.971667\n",
      "accuracy = 0.851064\n",
      "loss = 41.961895\n",
      "accuracy = 0.851064\n",
      "loss = 41.952103\n",
      "accuracy = 0.851064\n",
      "loss = 41.942402\n",
      "accuracy = 0.851064\n",
      "loss = 41.936342\n",
      "accuracy = 0.851064\n",
      "loss = 41.924201\n",
      "accuracy = 0.851064\n",
      "loss = 41.914679\n",
      "accuracy = 0.851064\n",
      "loss = 41.908706\n",
      "accuracy = 0.851064\n",
      "loss = 41.900746\n",
      "accuracy = 0.851064\n",
      "loss = 41.895153\n",
      "accuracy = 0.851064\n",
      "loss = 41.889661\n",
      "accuracy = 0.851064\n",
      "loss = 41.878816\n",
      "accuracy = 0.851064\n",
      "loss = 41.873432\n",
      "accuracy = 0.851064\n",
      "loss = 41.865636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.851064\n",
      "loss = 41.861599\n",
      "accuracy = 0.851064\n",
      "loss = 41.851404\n",
      "accuracy = 0.851064\n",
      "loss = 41.847823\n",
      "accuracy = 0.851064\n",
      "loss = 41.840528\n",
      "accuracy = 0.851064\n",
      "loss = 41.834330\n",
      "accuracy = 0.851064\n",
      "loss = 41.825937\n",
      "accuracy = 0.851064\n",
      "loss = 41.823291\n",
      "accuracy = 0.851064\n",
      "loss = 41.814570\n",
      "accuracy = 0.851064\n",
      "loss = 41.809285\n",
      "accuracy = 0.851064\n",
      "loss = 41.802791\n",
      "accuracy = 0.851064\n",
      "loss = 41.797602\n",
      "accuracy = 0.851064\n",
      "loss = 41.789381\n",
      "accuracy = 0.851064\n",
      "loss = 41.785900\n",
      "accuracy = 0.851064\n",
      "loss = 41.777325\n",
      "accuracy = 0.851064\n",
      "loss = 41.768877\n",
      "accuracy = 0.851064\n",
      "loss = 41.763859\n",
      "accuracy = 0.851064\n",
      "loss = 41.753573\n",
      "accuracy = 0.851064\n",
      "loss = 41.748588\n",
      "accuracy = 0.851064\n",
      "loss = 41.739101\n",
      "accuracy = 0.851064\n",
      "loss = 41.741477\n",
      "accuracy = 0.851064\n",
      "loss = 41.729418\n",
      "accuracy = 0.851064\n",
      "loss = 41.732946\n",
      "accuracy = 0.851064\n",
      "loss = 41.723726\n",
      "accuracy = 0.851064\n",
      "loss = 41.720938\n",
      "accuracy = 0.851064\n",
      "loss = 41.716676\n",
      "accuracy = 0.851064\n",
      "loss = 41.713957\n",
      "accuracy = 0.851064\n",
      "loss = 41.708699\n",
      "accuracy = 0.851064\n",
      "loss = 41.706004\n",
      "accuracy = 0.851064\n",
      "loss = 41.700864\n",
      "accuracy = 0.851064\n",
      "loss = 41.696762\n",
      "accuracy = 0.851064\n",
      "loss = 41.692645\n",
      "accuracy = 0.851064\n",
      "loss = 41.688514\n",
      "accuracy = 0.851064\n",
      "loss = 41.684384\n",
      "accuracy = 0.851064\n",
      "loss = 41.679782\n",
      "accuracy = 0.851064\n",
      "loss = 41.675165\n",
      "accuracy = 0.851064\n",
      "loss = 41.670734\n",
      "accuracy = 0.851064\n",
      "loss = 41.661271\n",
      "accuracy = 0.851064\n",
      "loss = 41.657114\n",
      "accuracy = 0.851064\n",
      "loss = 41.650385\n",
      "accuracy = 0.851064\n",
      "loss = 41.646776\n",
      "accuracy = 0.851064\n",
      "loss = 41.639186\n",
      "accuracy = 0.851064\n",
      "loss = 41.639999\n",
      "accuracy = 0.851064\n",
      "loss = 41.632779\n",
      "accuracy = 0.851064\n",
      "loss = 41.630349\n",
      "accuracy = 0.851064\n",
      "loss = 41.625801\n",
      "accuracy = 0.851064\n",
      "loss = 41.617656\n",
      "accuracy = 0.851064\n",
      "loss = 41.621075\n",
      "accuracy = 0.851064\n",
      "loss = 41.606032\n",
      "accuracy = 0.851064\n",
      "loss = 41.603949\n",
      "accuracy = 0.851064\n",
      "loss = 41.599424\n",
      "accuracy = 0.851064\n",
      "loss = 41.598582\n",
      "accuracy = 0.851064\n",
      "loss = 41.589514\n",
      "accuracy = 0.851064\n",
      "loss = 41.585667\n",
      "accuracy = 0.851064\n",
      "loss = 41.582373\n",
      "accuracy = 0.851064\n",
      "loss = 41.577320\n",
      "accuracy = 0.851064\n",
      "loss = 41.573734\n",
      "accuracy = 0.851064\n",
      "loss = 41.568104\n",
      "accuracy = 0.851064\n",
      "loss = 41.564612\n",
      "accuracy = 0.851064\n",
      "loss = 41.559866\n",
      "accuracy = 0.851064\n",
      "loss = 41.554531\n",
      "accuracy = 0.851064\n",
      "loss = 41.556963\n",
      "accuracy = 0.851064\n",
      "loss = 41.543040\n",
      "accuracy = 0.851064\n",
      "loss = 41.547788\n",
      "accuracy = 0.851064\n",
      "loss = 41.529727\n",
      "accuracy = 0.851064\n",
      "loss = 41.530859\n",
      "accuracy = 0.851064\n",
      "loss = 41.523968\n",
      "accuracy = 0.851064\n",
      "loss = 41.518210\n",
      "accuracy = 0.851064\n",
      "loss = 41.514543\n",
      "accuracy = 0.851064\n",
      "loss = 41.508147\n",
      "accuracy = 0.851064\n",
      "loss = 41.507557\n",
      "accuracy = 0.851064\n",
      "loss = 41.495196\n",
      "accuracy = 0.851064\n",
      "loss = 41.497470\n",
      "accuracy = 0.851064\n",
      "loss = 41.485030\n",
      "accuracy = 0.851064\n",
      "loss = 41.486077\n",
      "accuracy = 0.851064\n",
      "loss = 41.476414\n",
      "accuracy = 0.851064\n",
      "loss = 41.474240\n",
      "accuracy = 0.851064\n",
      "loss = 41.464245\n",
      "accuracy = 0.851064\n",
      "loss = 41.466396\n",
      "accuracy = 0.851064\n",
      "loss = 41.453127\n",
      "accuracy = 0.851064\n",
      "loss = 41.455654\n",
      "accuracy = 0.851064\n",
      "loss = 41.441987\n",
      "accuracy = 0.851064\n",
      "loss = 41.441802\n",
      "accuracy = 0.851064\n",
      "loss = 41.435781\n",
      "accuracy = 0.851064\n",
      "loss = 41.430032\n",
      "accuracy = 0.851064\n",
      "loss = 41.421892\n",
      "accuracy = 0.851064\n",
      "loss = 41.419568\n",
      "accuracy = 0.851064\n",
      "loss = 41.409900\n",
      "accuracy = 0.851064\n",
      "loss = 41.413854\n",
      "accuracy = 0.851064\n",
      "loss = 41.397943\n",
      "accuracy = 0.851064\n",
      "loss = 41.400682\n",
      "accuracy = 0.851064\n",
      "loss = 41.388984\n",
      "accuracy = 0.851064\n",
      "loss = 41.386589\n",
      "accuracy = 0.851064\n",
      "loss = 41.379250\n",
      "accuracy = 0.851064\n",
      "loss = 41.383784\n",
      "accuracy = 0.851064\n",
      "loss = 41.367217\n",
      "accuracy = 0.851064\n",
      "loss = 41.369925\n",
      "accuracy = 0.851064\n",
      "loss = 41.356717\n",
      "accuracy = 0.851064\n",
      "loss = 41.360460\n",
      "accuracy = 0.851064\n",
      "loss = 41.349905\n",
      "accuracy = 0.851064\n",
      "loss = 41.344901\n",
      "accuracy = 0.851064\n",
      "loss = 41.345461\n",
      "accuracy = 0.851064\n",
      "loss = 41.333644\n",
      "accuracy = 0.851064\n",
      "loss = 41.338075\n",
      "accuracy = 0.851064\n",
      "loss = 41.321644\n",
      "accuracy = 0.851064\n",
      "loss = 41.322371\n",
      "accuracy = 0.851064\n",
      "loss = 41.314834\n",
      "accuracy = 0.851064\n",
      "loss = 41.310437\n",
      "accuracy = 0.851064\n",
      "loss = 41.305217\n",
      "accuracy = 0.851064\n",
      "loss = 41.312901\n",
      "accuracy = 0.851064\n",
      "loss = 41.295837\n",
      "accuracy = 0.851064\n",
      "loss = 41.308527\n",
      "accuracy = 0.851064\n",
      "loss = 41.286864\n",
      "accuracy = 0.851064\n",
      "loss = 41.286951\n",
      "accuracy = 0.851064\n",
      "loss = 41.285722\n",
      "accuracy = 0.851064\n",
      "loss = 41.284662\n",
      "accuracy = 0.851064\n",
      "loss = 41.279735\n",
      "accuracy = 0.851064\n",
      "loss = 41.274694\n",
      "accuracy = 0.851064\n",
      "loss = 41.277152\n",
      "accuracy = 0.851064\n",
      "loss = 41.265742\n",
      "accuracy = 0.851064\n",
      "loss = 41.270158\n",
      "accuracy = 0.851064\n",
      "loss = 41.260184\n",
      "accuracy = 0.851064\n",
      "loss = 41.265617\n",
      "accuracy = 0.851064\n",
      "loss = 41.252975\n",
      "accuracy = 0.851064\n",
      "loss = 41.254571\n",
      "accuracy = 0.851064\n",
      "loss = 41.247410\n",
      "accuracy = 0.851064\n",
      "loss = 41.249397\n",
      "accuracy = 0.851064\n",
      "loss = 41.240412\n",
      "accuracy = 0.851064\n",
      "loss = 41.243638\n",
      "accuracy = 0.851064\n",
      "loss = 41.232879\n",
      "accuracy = 0.851064\n",
      "loss = 41.240313\n",
      "accuracy = 0.851064\n",
      "loss = 41.227424\n",
      "accuracy = 0.851064\n",
      "loss = 41.236295\n",
      "accuracy = 0.851064\n",
      "loss = 41.216989\n",
      "accuracy = 0.851064\n",
      "loss = 41.225454\n",
      "accuracy = 0.851064\n",
      "loss = 41.211102\n",
      "accuracy = 0.851064\n",
      "loss = 41.211650\n",
      "accuracy = 0.851064\n",
      "loss = 41.204054\n",
      "accuracy = 0.851064\n",
      "loss = 41.210027\n",
      "accuracy = 0.851064\n",
      "loss = 41.195049\n",
      "accuracy = 0.851064\n",
      "loss = 41.202111\n",
      "accuracy = 0.851064\n",
      "loss = 41.187899\n",
      "accuracy = 0.851064\n",
      "loss = 41.189633\n",
      "accuracy = 0.851064\n",
      "loss = 41.185407\n",
      "accuracy = 0.851064\n",
      "loss = 41.187503\n",
      "accuracy = 0.851064\n",
      "loss = 41.175643\n",
      "accuracy = 0.851064\n",
      "loss = 41.180626\n",
      "accuracy = 0.851064\n",
      "loss = 41.165828\n",
      "accuracy = 0.851064\n",
      "loss = 41.165991\n",
      "accuracy = 0.851064\n",
      "loss = 41.165455\n",
      "accuracy = 0.851064\n",
      "loss = 41.162997\n",
      "accuracy = 0.851064\n",
      "loss = 41.155863\n",
      "accuracy = 0.851064\n",
      "loss = 41.155900\n",
      "accuracy = 0.851064\n",
      "loss = 41.143488\n",
      "accuracy = 0.851064\n",
      "loss = 41.144443\n",
      "accuracy = 0.851064\n",
      "loss = 41.140657\n",
      "accuracy = 0.851064\n",
      "loss = 41.141213\n",
      "accuracy = 0.851064\n",
      "loss = 41.132038\n",
      "accuracy = 0.851064\n",
      "loss = 41.133777\n",
      "accuracy = 0.851064\n",
      "loss = 41.126740\n",
      "accuracy = 0.851064\n",
      "loss = 41.126935\n",
      "accuracy = 0.851064\n",
      "loss = 41.119995\n",
      "accuracy = 0.851064\n",
      "loss = 41.121295\n",
      "accuracy = 0.851064\n",
      "loss = 41.107578\n",
      "accuracy = 0.851064\n",
      "loss = 41.107325\n",
      "accuracy = 0.851064\n",
      "loss = 41.103664\n",
      "accuracy = 0.851064\n",
      "loss = 41.099442\n",
      "accuracy = 0.851064\n",
      "loss = 41.099760\n",
      "accuracy = 0.851064\n",
      "loss = 41.093246\n",
      "accuracy = 0.851064\n",
      "loss = 41.095031\n",
      "accuracy = 0.851064\n",
      "loss = 41.086067\n",
      "accuracy = 0.851064\n",
      "loss = 41.089931\n",
      "accuracy = 0.851064\n",
      "loss = 41.078088\n",
      "accuracy = 0.851064\n",
      "loss = 41.082736\n",
      "accuracy = 0.851064\n",
      "loss = 41.073990\n",
      "accuracy = 0.851064\n",
      "loss = 41.078087\n",
      "accuracy = 0.851064\n",
      "loss = 41.067650\n",
      "accuracy = 0.851064\n",
      "loss = 41.072119\n",
      "accuracy = 0.851064\n",
      "loss = 41.063454\n",
      "accuracy = 0.851064\n",
      "loss = 41.067574\n",
      "accuracy = 0.851064\n",
      "loss = 41.057134\n",
      "accuracy = 0.851064\n",
      "loss = 41.058535\n",
      "accuracy = 0.851064\n",
      "loss = 41.051323\n",
      "accuracy = 0.851064\n",
      "loss = 41.054443\n",
      "accuracy = 0.851064\n",
      "loss = 41.044145\n",
      "accuracy = 0.851064\n",
      "loss = 41.048824\n",
      "accuracy = 0.851064\n",
      "loss = 41.037975\n",
      "accuracy = 0.851064\n",
      "loss = 41.042518\n",
      "accuracy = 0.851064\n",
      "loss = 41.032221\n",
      "accuracy = 0.851064\n",
      "loss = 41.033817\n",
      "accuracy = 0.851064\n",
      "loss = 41.026254\n",
      "accuracy = 0.851064\n",
      "loss = 41.031075\n",
      "accuracy = 0.851064\n",
      "loss = 41.021659\n",
      "accuracy = 0.851064\n",
      "loss = 41.024200\n",
      "accuracy = 0.851064\n",
      "loss = 41.019467\n",
      "accuracy = 0.851064\n",
      "loss = 41.019050\n",
      "accuracy = 0.851064\n",
      "loss = 41.014223\n",
      "accuracy = 0.851064\n",
      "loss = 41.012978\n",
      "accuracy = 0.851064\n",
      "loss = 41.011112\n",
      "accuracy = 0.851064\n",
      "loss = 41.012088\n",
      "accuracy = 0.851064\n",
      "loss = 41.004970\n",
      "accuracy = 0.851064\n",
      "loss = 41.010246\n",
      "accuracy = 0.851064\n",
      "loss = 40.999535\n",
      "accuracy = 0.851064\n",
      "loss = 41.004820\n",
      "accuracy = 0.851064\n",
      "loss = 40.996042\n",
      "accuracy = 0.851064\n",
      "loss = 41.003664\n",
      "accuracy = 0.851064\n",
      "loss = 40.988661\n",
      "accuracy = 0.851064\n",
      "loss = 40.988287\n",
      "accuracy = 0.851064\n",
      "loss = 40.987169\n",
      "accuracy = 0.851064\n",
      "loss = 40.983229\n",
      "accuracy = 0.851064\n",
      "loss = 40.981758\n",
      "accuracy = 0.851064\n",
      "loss = 40.977357\n",
      "accuracy = 0.851064\n",
      "loss = 40.978475\n",
      "accuracy = 0.851064\n",
      "loss = 40.971932\n",
      "accuracy = 0.851064\n",
      "loss = 40.972755\n",
      "accuracy = 0.851064\n",
      "loss = 40.970479\n",
      "accuracy = 0.851064\n",
      "loss = 40.969028\n",
      "accuracy = 0.851064\n",
      "loss = 40.966975\n",
      "accuracy = 0.851064\n",
      "loss = 40.966141\n",
      "accuracy = 0.851064\n",
      "loss = 40.959552\n",
      "accuracy = 0.851064\n",
      "loss = 40.959821\n",
      "accuracy = 0.851064\n",
      "loss = 40.957569\n",
      "accuracy = 0.851064\n",
      "loss = 40.959049\n",
      "accuracy = 0.851064\n",
      "loss = 40.952556\n",
      "accuracy = 0.851064\n",
      "loss = 40.956511\n",
      "accuracy = 0.851064\n",
      "loss = 40.946619\n",
      "accuracy = 0.851064\n",
      "loss = 40.946760\n",
      "accuracy = 0.851064\n",
      "loss = 40.944334\n",
      "accuracy = 0.851064\n",
      "loss = 40.942710\n",
      "accuracy = 0.851064\n",
      "loss = 40.940693\n",
      "accuracy = 0.851064\n",
      "loss = 40.940171\n",
      "accuracy = 0.851064\n",
      "loss = 40.934557\n",
      "accuracy = 0.851064\n",
      "loss = 40.944669\n",
      "accuracy = 0.851064\n",
      "loss = 40.928751\n",
      "accuracy = 0.851064\n",
      "loss = 40.928663\n",
      "accuracy = 0.851064\n",
      "loss = 40.924850\n",
      "accuracy = 0.851064\n",
      "loss = 40.925000\n",
      "accuracy = 0.851064\n",
      "loss = 40.922765\n",
      "accuracy = 0.851064\n",
      "loss = 40.922116\n",
      "accuracy = 0.851064\n",
      "loss = 40.917724\n",
      "accuracy = 0.851064\n",
      "loss = 40.918664\n",
      "accuracy = 0.851064\n",
      "loss = 40.912325\n",
      "accuracy = 0.851064\n",
      "loss = 40.912299\n",
      "accuracy = 0.851064\n",
      "loss = 40.908833\n",
      "accuracy = 0.851064\n",
      "loss = 40.908418\n",
      "accuracy = 0.851064\n",
      "loss = 40.902834\n",
      "accuracy = 0.851064\n",
      "loss = 40.901930\n",
      "accuracy = 0.851064\n",
      "loss = 40.900232\n",
      "accuracy = 0.851064\n",
      "loss = 40.896643\n",
      "accuracy = 0.851064\n",
      "loss = 40.896263\n",
      "accuracy = 0.851064\n",
      "loss = 40.890775\n",
      "accuracy = 0.851064\n",
      "loss = 40.892638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.851064\n",
      "loss = 40.885129\n",
      "accuracy = 0.851064\n",
      "loss = 40.885608\n",
      "accuracy = 0.851064\n",
      "loss = 40.879414\n",
      "accuracy = 0.851064\n",
      "loss = 40.879207\n",
      "accuracy = 0.851064\n",
      "loss = 40.876489\n",
      "accuracy = 0.851064\n",
      "loss = 40.873951\n",
      "accuracy = 0.851064\n",
      "loss = 40.871466\n",
      "accuracy = 0.851064\n",
      "loss = 40.867447\n",
      "accuracy = 0.851064\n",
      "loss = 40.867204\n",
      "accuracy = 0.852998\n",
      "loss = 40.861917\n",
      "accuracy = 0.852998\n",
      "loss = 101.836466\n",
      "accuracy = 0.497110\n",
      "loss = 100.285926\n",
      "accuracy = 0.491329\n",
      "loss = 98.766239\n",
      "accuracy = 0.491329\n",
      "loss = 97.255899\n",
      "accuracy = 0.485549\n",
      "loss = 95.772164\n",
      "accuracy = 0.485549\n",
      "loss = 94.295777\n",
      "accuracy = 0.473988\n",
      "loss = 92.875056\n",
      "accuracy = 0.473988\n",
      "loss = 91.456135\n",
      "accuracy = 0.468208\n",
      "loss = 90.059881\n",
      "accuracy = 0.456647\n",
      "loss = 88.690328\n",
      "accuracy = 0.456647\n",
      "loss = 87.339356\n",
      "accuracy = 0.456647\n",
      "loss = 85.990050\n",
      "accuracy = 0.450867\n",
      "loss = 84.649755\n",
      "accuracy = 0.445087\n",
      "loss = 83.338443\n",
      "accuracy = 0.439306\n",
      "loss = 82.028730\n",
      "accuracy = 0.445087\n",
      "loss = 80.736893\n",
      "accuracy = 0.450867\n",
      "loss = 79.471264\n",
      "accuracy = 0.445087\n",
      "loss = 78.236952\n",
      "accuracy = 0.445087\n",
      "loss = 77.018105\n",
      "accuracy = 0.445087\n",
      "loss = 75.802615\n",
      "accuracy = 0.433526\n",
      "loss = 74.642638\n",
      "accuracy = 0.445087\n",
      "loss = 73.505404\n",
      "accuracy = 0.445087\n",
      "loss = 72.378143\n",
      "accuracy = 0.445087\n",
      "loss = 71.273924\n",
      "accuracy = 0.445087\n",
      "loss = 70.176983\n",
      "accuracy = 0.456647\n",
      "loss = 69.086801\n",
      "accuracy = 0.462428\n",
      "loss = 68.038457\n",
      "accuracy = 0.468208\n",
      "loss = 67.013429\n",
      "accuracy = 0.468208\n",
      "loss = 65.997210\n",
      "accuracy = 0.468208\n",
      "loss = 64.995949\n",
      "accuracy = 0.462428\n",
      "loss = 63.995650\n",
      "accuracy = 0.468208\n",
      "loss = 62.996315\n",
      "accuracy = 0.473988\n",
      "loss = 61.997944\n",
      "accuracy = 0.485549\n",
      "loss = 61.010823\n",
      "accuracy = 0.485549\n",
      "loss = 60.060383\n",
      "accuracy = 0.479769\n",
      "loss = 59.124196\n",
      "accuracy = 0.485549\n",
      "loss = 58.220130\n",
      "accuracy = 0.497110\n",
      "loss = 57.320075\n",
      "accuracy = 0.491329\n",
      "loss = 56.429589\n",
      "accuracy = 0.508671\n",
      "loss = 55.539951\n",
      "accuracy = 0.520231\n",
      "loss = 54.651165\n",
      "accuracy = 0.531792\n",
      "loss = 53.763881\n",
      "accuracy = 0.543353\n",
      "loss = 52.874076\n",
      "accuracy = 0.549133\n",
      "loss = 51.985113\n",
      "accuracy = 0.543353\n",
      "loss = 51.099937\n",
      "accuracy = 0.554913\n",
      "loss = 50.218643\n",
      "accuracy = 0.560694\n",
      "loss = 49.349604\n",
      "accuracy = 0.560694\n",
      "loss = 48.481387\n",
      "accuracy = 0.560694\n",
      "loss = 47.625439\n",
      "accuracy = 0.572254\n",
      "loss = 46.799644\n",
      "accuracy = 0.572254\n",
      "loss = 45.974622\n",
      "accuracy = 0.583815\n",
      "loss = 45.156999\n",
      "accuracy = 0.589595\n",
      "loss = 44.347977\n",
      "accuracy = 0.601156\n",
      "loss = 43.562795\n",
      "accuracy = 0.601156\n",
      "loss = 42.798213\n",
      "accuracy = 0.612717\n",
      "loss = 42.073263\n",
      "accuracy = 0.612717\n",
      "loss = 41.391659\n",
      "accuracy = 0.618497\n",
      "loss = 40.723193\n",
      "accuracy = 0.618497\n",
      "loss = 40.080646\n",
      "accuracy = 0.624277\n",
      "loss = 39.468239\n",
      "accuracy = 0.630058\n",
      "loss = 38.871092\n",
      "accuracy = 0.635838\n",
      "loss = 38.310127\n",
      "accuracy = 0.647399\n",
      "loss = 37.797021\n",
      "accuracy = 0.647399\n",
      "loss = 37.285162\n",
      "accuracy = 0.647399\n",
      "loss = 36.782184\n",
      "accuracy = 0.658960\n",
      "loss = 36.279676\n",
      "accuracy = 0.670520\n",
      "loss = 35.777643\n",
      "accuracy = 0.670520\n",
      "loss = 35.278667\n",
      "accuracy = 0.670520\n",
      "loss = 34.799020\n",
      "accuracy = 0.676301\n",
      "loss = 34.329502\n",
      "accuracy = 0.693642\n",
      "loss = 33.891586\n",
      "accuracy = 0.693642\n",
      "loss = 33.505939\n",
      "accuracy = 0.699422\n",
      "loss = 33.128728\n",
      "accuracy = 0.693642\n",
      "loss = 32.755541\n",
      "accuracy = 0.699422\n",
      "loss = 32.401326\n",
      "accuracy = 0.705202\n",
      "loss = 32.050545\n",
      "accuracy = 0.705202\n",
      "loss = 31.727261\n",
      "accuracy = 0.705202\n",
      "loss = 31.427797\n",
      "accuracy = 0.705202\n",
      "loss = 31.142188\n",
      "accuracy = 0.716763\n",
      "loss = 30.856859\n",
      "accuracy = 0.722543\n",
      "loss = 30.576165\n",
      "accuracy = 0.745665\n",
      "loss = 30.304081\n",
      "accuracy = 0.745665\n",
      "loss = 30.032896\n",
      "accuracy = 0.768786\n",
      "loss = 29.780280\n",
      "accuracy = 0.763006\n",
      "loss = 29.526760\n",
      "accuracy = 0.763006\n",
      "loss = 29.274983\n",
      "accuracy = 0.763006\n",
      "loss = 29.023447\n",
      "accuracy = 0.768786\n",
      "loss = 28.776935\n",
      "accuracy = 0.774566\n",
      "loss = 28.544934\n",
      "accuracy = 0.780347\n",
      "loss = 28.325853\n",
      "accuracy = 0.786127\n",
      "loss = 28.114884\n",
      "accuracy = 0.786127\n",
      "loss = 27.909908\n",
      "accuracy = 0.786127\n",
      "loss = 27.705116\n",
      "accuracy = 0.797688\n",
      "loss = 27.500509\n",
      "accuracy = 0.803468\n",
      "loss = 27.298292\n",
      "accuracy = 0.803468\n",
      "loss = 27.131642\n",
      "accuracy = 0.803468\n",
      "loss = 26.971495\n",
      "accuracy = 0.803468\n",
      "loss = 26.821807\n",
      "accuracy = 0.803468\n",
      "loss = 26.673887\n",
      "accuracy = 0.803468\n",
      "loss = 26.531468\n",
      "accuracy = 0.803468\n",
      "loss = 26.389449\n",
      "accuracy = 0.809249\n",
      "loss = 26.252432\n",
      "accuracy = 0.809249\n",
      "loss = 26.115554\n",
      "accuracy = 0.809249\n",
      "loss = 25.978816\n",
      "accuracy = 0.809249\n",
      "loss = 25.846125\n",
      "accuracy = 0.809249\n",
      "loss = 25.727159\n",
      "accuracy = 0.809249\n",
      "loss = 25.616192\n",
      "accuracy = 0.815029\n",
      "loss = 25.505339\n",
      "accuracy = 0.815029\n",
      "loss = 25.397032\n",
      "accuracy = 0.815029\n",
      "loss = 25.292932\n",
      "accuracy = 0.820809\n",
      "loss = 25.188942\n",
      "accuracy = 0.826590\n",
      "loss = 25.085066\n",
      "accuracy = 0.826590\n",
      "loss = 24.981303\n",
      "accuracy = 0.826590\n",
      "loss = 24.877655\n",
      "accuracy = 0.832370\n",
      "loss = 24.774123\n",
      "accuracy = 0.832370\n",
      "loss = 24.672752\n",
      "accuracy = 0.832370\n",
      "loss = 24.570815\n",
      "accuracy = 0.838150\n",
      "loss = 24.475260\n",
      "accuracy = 0.838150\n",
      "loss = 24.380309\n",
      "accuracy = 0.838150\n",
      "loss = 24.290312\n",
      "accuracy = 0.838150\n",
      "loss = 24.208418\n",
      "accuracy = 0.838150\n",
      "loss = 24.136912\n",
      "accuracy = 0.838150\n",
      "loss = 24.066563\n",
      "accuracy = 0.838150\n",
      "loss = 23.996777\n",
      "accuracy = 0.838150\n",
      "loss = 23.928872\n",
      "accuracy = 0.843931\n",
      "loss = 23.861028\n",
      "accuracy = 0.843931\n",
      "loss = 23.793979\n",
      "accuracy = 0.843931\n",
      "loss = 23.731826\n",
      "accuracy = 0.843931\n",
      "loss = 23.669740\n",
      "accuracy = 0.843931\n",
      "loss = 23.607721\n",
      "accuracy = 0.843931\n",
      "loss = 23.545770\n",
      "accuracy = 0.843931\n",
      "loss = 23.483888\n",
      "accuracy = 0.843931\n",
      "loss = 23.422077\n",
      "accuracy = 0.843931\n",
      "loss = 23.360336\n",
      "accuracy = 0.843931\n",
      "loss = 23.298793\n",
      "accuracy = 0.843931\n",
      "loss = 23.245582\n",
      "accuracy = 0.843931\n",
      "loss = 23.199226\n",
      "accuracy = 0.843931\n",
      "loss = 23.152933\n",
      "accuracy = 0.843931\n",
      "loss = 23.106714\n",
      "accuracy = 0.843931\n",
      "loss = 23.058687\n",
      "accuracy = 0.843931\n",
      "loss = 23.015657\n",
      "accuracy = 0.843931\n",
      "loss = 22.970044\n",
      "accuracy = 0.843931\n",
      "loss = 22.929195\n",
      "accuracy = 0.843931\n",
      "loss = 22.888554\n",
      "accuracy = 0.843931\n",
      "loss = 22.843237\n",
      "accuracy = 0.843931\n",
      "loss = 22.802542\n",
      "accuracy = 0.843931\n",
      "loss = 22.763747\n",
      "accuracy = 0.843931\n",
      "loss = 22.718613\n",
      "accuracy = 0.843931\n",
      "loss = 22.678386\n",
      "accuracy = 0.843931\n",
      "loss = 22.638206\n",
      "accuracy = 0.843931\n",
      "loss = 22.601166\n",
      "accuracy = 0.843931\n",
      "loss = 22.573606\n",
      "accuracy = 0.843931\n",
      "loss = 22.548193\n",
      "accuracy = 0.843931\n",
      "loss = 22.521541\n",
      "accuracy = 0.843931\n",
      "loss = 22.494913\n",
      "accuracy = 0.843931\n",
      "loss = 22.468310\n",
      "accuracy = 0.843931\n",
      "loss = 22.441732\n",
      "accuracy = 0.843931\n",
      "loss = 22.415180\n",
      "accuracy = 0.843931\n",
      "loss = 22.388654\n",
      "accuracy = 0.843931\n",
      "loss = 22.362154\n",
      "accuracy = 0.843931\n",
      "loss = 22.335680\n",
      "accuracy = 0.843931\n",
      "loss = 22.309234\n",
      "accuracy = 0.843931\n",
      "loss = 22.284603\n",
      "accuracy = 0.843931\n",
      "loss = 22.264921\n",
      "accuracy = 0.843931\n",
      "loss = 22.245258\n",
      "accuracy = 0.843931\n",
      "loss = 22.225612\n",
      "accuracy = 0.843931\n",
      "loss = 22.206083\n",
      "accuracy = 0.843931\n",
      "loss = 22.193748\n",
      "accuracy = 0.843931\n",
      "loss = 22.181431\n",
      "accuracy = 0.843931\n",
      "loss = 22.169156\n",
      "accuracy = 0.843931\n",
      "loss = 22.150325\n",
      "accuracy = 0.843931\n",
      "loss = 22.138063\n",
      "accuracy = 0.843931\n",
      "loss = 22.125821\n",
      "accuracy = 0.843931\n",
      "loss = 22.113596\n",
      "accuracy = 0.843931\n",
      "loss = 22.101471\n",
      "accuracy = 0.843931\n",
      "loss = 22.085102\n",
      "accuracy = 0.843931\n",
      "loss = 22.073565\n",
      "accuracy = 0.843931\n",
      "loss = 22.062357\n",
      "accuracy = 0.843931\n",
      "loss = 22.046932\n",
      "accuracy = 0.843931\n",
      "loss = 22.034024\n",
      "accuracy = 0.843931\n",
      "loss = 22.021759\n",
      "accuracy = 0.843931\n",
      "loss = 22.012231\n",
      "accuracy = 0.843931\n",
      "loss = 22.002712\n",
      "accuracy = 0.843931\n",
      "loss = 21.993203\n",
      "accuracy = 0.843931\n",
      "loss = 21.983705\n",
      "accuracy = 0.843931\n",
      "loss = 21.974216\n",
      "accuracy = 0.843931\n",
      "loss = 21.964738\n",
      "accuracy = 0.843931\n",
      "loss = 21.955271\n",
      "accuracy = 0.843931\n",
      "loss = 21.945815\n",
      "accuracy = 0.843931\n",
      "loss = 21.936370\n",
      "accuracy = 0.843931\n",
      "loss = 21.927762\n",
      "accuracy = 0.843931\n",
      "loss = 21.916537\n",
      "accuracy = 0.843931\n",
      "loss = 21.905319\n",
      "accuracy = 0.843931\n",
      "loss = 21.894108\n",
      "accuracy = 0.843931\n",
      "loss = 21.882904\n",
      "accuracy = 0.843931\n",
      "loss = 21.871708\n",
      "accuracy = 0.843931\n",
      "loss = 21.860520\n",
      "accuracy = 0.843931\n",
      "loss = 21.849861\n",
      "accuracy = 0.843931\n",
      "loss = 21.838537\n",
      "accuracy = 0.843931\n",
      "loss = 21.827219\n",
      "accuracy = 0.849711\n",
      "loss = 21.815909\n",
      "accuracy = 0.849711\n",
      "loss = 21.804606\n",
      "accuracy = 0.849711\n",
      "loss = 21.793311\n",
      "accuracy = 0.849711\n",
      "loss = 21.782024\n",
      "accuracy = 0.849711\n",
      "loss = 21.770745\n",
      "accuracy = 0.849711\n",
      "loss = 21.759474\n",
      "accuracy = 0.849711\n",
      "loss = 21.748331\n",
      "accuracy = 0.849711\n",
      "loss = 21.738329\n",
      "accuracy = 0.849711\n",
      "loss = 21.728339\n",
      "accuracy = 0.843931\n",
      "loss = 21.718415\n",
      "accuracy = 0.843931\n",
      "loss = 21.707751\n",
      "accuracy = 0.843931\n",
      "loss = 21.697796\n",
      "accuracy = 0.843931\n",
      "loss = 21.687854\n",
      "accuracy = 0.843931\n",
      "loss = 21.677925\n",
      "accuracy = 0.843931\n",
      "loss = 21.668010\n",
      "accuracy = 0.843931\n",
      "loss = 21.658108\n",
      "accuracy = 0.843931\n",
      "loss = 21.648220\n",
      "accuracy = 0.843931\n",
      "loss = 21.638718\n",
      "accuracy = 0.843931\n",
      "loss = 21.626498\n",
      "accuracy = 0.843931\n",
      "loss = 21.614286\n",
      "accuracy = 0.843931\n",
      "loss = 21.602083\n",
      "accuracy = 0.843931\n",
      "loss = 21.589889\n",
      "accuracy = 0.843931\n",
      "loss = 21.577862\n",
      "accuracy = 0.843931\n",
      "loss = 21.570093\n",
      "accuracy = 0.843931\n",
      "loss = 21.558339\n",
      "accuracy = 0.849711\n",
      "loss = 21.550536\n",
      "accuracy = 0.849711\n",
      "loss = 21.542742\n",
      "accuracy = 0.849711\n",
      "loss = 21.535161\n",
      "accuracy = 0.849711\n",
      "loss = 21.523286\n",
      "accuracy = 0.849711\n",
      "loss = 21.515520\n",
      "accuracy = 0.849711\n",
      "loss = 21.507910\n",
      "accuracy = 0.849711\n",
      "loss = 21.496482\n",
      "accuracy = 0.849711\n",
      "loss = 21.490455\n",
      "accuracy = 0.855491\n",
      "loss = 21.479105\n",
      "accuracy = 0.855491\n",
      "loss = 21.473548\n",
      "accuracy = 0.855491\n",
      "loss = 21.456496\n",
      "accuracy = 0.855491\n",
      "loss = 21.450375\n",
      "accuracy = 0.855491\n",
      "loss = 21.439471\n",
      "accuracy = 0.855491\n",
      "loss = 21.427866\n",
      "accuracy = 0.855491\n",
      "loss = 21.422207\n",
      "accuracy = 0.855491\n",
      "loss = 21.410485\n",
      "accuracy = 0.855491\n",
      "loss = 21.398813\n",
      "accuracy = 0.855491\n",
      "loss = 21.388873\n",
      "accuracy = 0.855491\n",
      "loss = 21.374488\n",
      "accuracy = 0.855491\n",
      "loss = 21.366154\n",
      "accuracy = 0.855491\n",
      "loss = 21.364732\n",
      "accuracy = 0.855491\n",
      "loss = 21.342999\n",
      "accuracy = 0.855491\n",
      "loss = 21.339570\n",
      "accuracy = 0.855491\n",
      "loss = 21.323674\n",
      "accuracy = 0.855491\n",
      "loss = 21.320243\n",
      "accuracy = 0.855491\n",
      "loss = 21.304030\n",
      "accuracy = 0.855491\n",
      "loss = 21.300392\n",
      "accuracy = 0.855491\n",
      "loss = 21.296999\n",
      "accuracy = 0.849711\n",
      "loss = 21.280767\n",
      "accuracy = 0.855491\n",
      "loss = 21.277818\n",
      "accuracy = 0.849711\n",
      "loss = 21.261221\n",
      "accuracy = 0.849711\n",
      "loss = 21.257926\n",
      "accuracy = 0.849711\n",
      "loss = 21.247531\n",
      "accuracy = 0.849711\n",
      "loss = 21.243166\n",
      "accuracy = 0.849711\n",
      "loss = 21.231591\n",
      "accuracy = 0.849711\n",
      "loss = 21.231041\n",
      "accuracy = 0.849711\n",
      "loss = 21.219102\n",
      "accuracy = 0.849711\n",
      "loss = 21.212535\n",
      "accuracy = 0.849711\n",
      "loss = 21.206044\n",
      "accuracy = 0.849711\n",
      "loss = 21.195245\n",
      "accuracy = 0.849711\n",
      "loss = 21.195925\n",
      "accuracy = 0.849711\n",
      "loss = 21.184240\n",
      "accuracy = 0.849711\n",
      "loss = 21.178976\n",
      "accuracy = 0.849711\n",
      "loss = 21.167730\n",
      "accuracy = 0.849711\n",
      "loss = 21.162280\n",
      "accuracy = 0.849711\n",
      "loss = 21.157286\n",
      "accuracy = 0.849711\n",
      "loss = 21.145803\n",
      "accuracy = 0.849711\n",
      "loss = 21.140577\n",
      "accuracy = 0.855491\n",
      "loss = 21.130210\n",
      "accuracy = 0.855491\n",
      "loss = 21.133076\n",
      "accuracy = 0.855491\n",
      "loss = 21.116208\n",
      "accuracy = 0.855491\n",
      "loss = 21.111134\n",
      "accuracy = 0.855491\n",
      "loss = 21.100632\n",
      "accuracy = 0.855491\n",
      "loss = 21.090668\n",
      "accuracy = 0.855491\n",
      "loss = 21.092890\n",
      "accuracy = 0.855491\n",
      "loss = 21.076635\n",
      "accuracy = 0.855491\n",
      "loss = 21.078670\n",
      "accuracy = 0.855491\n",
      "loss = 21.067102\n",
      "accuracy = 0.855491\n",
      "loss = 21.056931\n",
      "accuracy = 0.855491\n",
      "loss = 21.052830\n",
      "accuracy = 0.855491\n",
      "loss = 21.039374\n",
      "accuracy = 0.855491\n",
      "loss = 21.037956\n",
      "accuracy = 0.855491\n",
      "loss = 21.029649\n",
      "accuracy = 0.855491\n",
      "loss = 21.028533\n",
      "accuracy = 0.855491\n",
      "loss = 21.020169\n",
      "accuracy = 0.855491\n",
      "loss = 21.011817\n",
      "accuracy = 0.855491\n",
      "loss = 21.010862\n",
      "accuracy = 0.855491\n",
      "loss = 20.997508\n",
      "accuracy = 0.855491\n",
      "loss = 21.001611\n",
      "accuracy = 0.855491\n",
      "loss = 20.993477\n",
      "accuracy = 0.855491\n",
      "loss = 20.982232\n",
      "accuracy = 0.855491\n",
      "loss = 20.983656\n",
      "accuracy = 0.855491\n",
      "loss = 20.977889\n",
      "accuracy = 0.855491\n",
      "loss = 20.972130\n",
      "accuracy = 0.855491\n",
      "loss = 20.966378\n",
      "accuracy = 0.855491\n",
      "loss = 20.960633\n",
      "accuracy = 0.855491\n",
      "loss = 20.954895\n",
      "accuracy = 0.855491\n",
      "loss = 20.949221\n",
      "accuracy = 0.855491\n",
      "loss = 20.950658\n",
      "accuracy = 0.855491\n",
      "loss = 20.944940\n",
      "accuracy = 0.855491\n",
      "loss = 20.939230\n",
      "accuracy = 0.855491\n",
      "loss = 20.933528\n",
      "accuracy = 0.855491\n",
      "loss = 20.927834\n",
      "accuracy = 0.855491\n",
      "loss = 20.922148\n",
      "accuracy = 0.855491\n",
      "loss = 20.916571\n",
      "accuracy = 0.855491\n",
      "loss = 20.918022\n",
      "accuracy = 0.855491\n",
      "loss = 20.912358\n",
      "accuracy = 0.855491\n",
      "loss = 20.906703\n",
      "accuracy = 0.855491\n",
      "loss = 20.901057\n",
      "accuracy = 0.855491\n",
      "loss = 20.895419\n",
      "accuracy = 0.855491\n",
      "loss = 20.889827\n",
      "accuracy = 0.855491\n",
      "loss = 20.891396\n",
      "accuracy = 0.855491\n",
      "loss = 20.885782\n",
      "accuracy = 0.855491\n",
      "loss = 20.880179\n",
      "accuracy = 0.855491\n",
      "loss = 20.874584\n",
      "accuracy = 0.855491\n",
      "loss = 20.868999\n",
      "accuracy = 0.855491\n",
      "loss = 20.863424\n",
      "accuracy = 0.855491\n",
      "loss = 20.857941\n",
      "accuracy = 0.855491\n",
      "loss = 20.859531\n",
      "accuracy = 0.849711\n",
      "loss = 20.853982\n",
      "accuracy = 0.849711\n",
      "loss = 20.848444\n",
      "accuracy = 0.849711\n",
      "loss = 20.842915\n",
      "accuracy = 0.849711\n",
      "loss = 20.837397\n",
      "accuracy = 0.849711\n",
      "loss = 20.832069\n",
      "accuracy = 0.849711\n",
      "loss = 20.837059\n",
      "accuracy = 0.849711\n",
      "loss = 20.827376\n",
      "accuracy = 0.849711\n",
      "loss = 20.824997\n",
      "accuracy = 0.849711\n",
      "loss = 20.817989\n",
      "accuracy = 0.849711\n",
      "loss = 20.815721\n",
      "accuracy = 0.849711\n",
      "loss = 20.808651\n",
      "accuracy = 0.849711\n",
      "loss = 20.806496\n",
      "accuracy = 0.849711\n",
      "loss = 20.799438\n",
      "accuracy = 0.849711\n",
      "loss = 20.792604\n",
      "accuracy = 0.849711\n",
      "loss = 20.790286\n",
      "accuracy = 0.849711\n",
      "loss = 20.783499\n",
      "accuracy = 0.849711\n",
      "loss = 20.786138\n",
      "accuracy = 0.843931\n",
      "loss = 20.779557\n",
      "accuracy = 0.843931\n",
      "loss = 20.777599\n",
      "accuracy = 0.849711\n",
      "loss = 20.775650\n",
      "accuracy = 0.849711\n",
      "loss = 20.773710\n",
      "accuracy = 0.849711\n",
      "loss = 20.771778\n",
      "accuracy = 0.849711\n",
      "loss = 20.769856\n",
      "accuracy = 0.849711\n",
      "loss = 20.767943\n",
      "accuracy = 0.849711\n",
      "loss = 20.766040\n",
      "accuracy = 0.849711\n",
      "loss = 20.764146\n",
      "accuracy = 0.849711\n",
      "loss = 20.762261\n",
      "accuracy = 0.849711\n",
      "loss = 20.760387\n",
      "accuracy = 0.849711\n",
      "loss = 20.758522\n",
      "accuracy = 0.849711\n",
      "loss = 20.756666\n",
      "accuracy = 0.849711\n",
      "loss = 20.754821\n",
      "accuracy = 0.849711\n",
      "loss = 20.753076\n",
      "accuracy = 0.849711\n",
      "loss = 20.745484\n",
      "accuracy = 0.849711\n",
      "loss = 20.743671\n",
      "accuracy = 0.849711\n",
      "loss = 20.742190\n",
      "accuracy = 0.849711\n",
      "loss = 20.734397\n",
      "accuracy = 0.849711\n",
      "loss = 20.732843\n",
      "accuracy = 0.849711\n",
      "loss = 20.725168\n",
      "accuracy = 0.849711\n",
      "loss = 20.723542\n",
      "accuracy = 0.849711\n",
      "loss = 20.715985\n",
      "accuracy = 0.849711\n",
      "loss = 20.714288\n",
      "accuracy = 0.849711\n",
      "loss = 20.706848\n",
      "accuracy = 0.849711\n",
      "loss = 20.705140\n",
      "accuracy = 0.849711\n",
      "loss = 20.703717\n",
      "accuracy = 0.849711\n",
      "loss = 20.696072\n",
      "accuracy = 0.849711\n",
      "loss = 20.694583\n",
      "accuracy = 0.849711\n",
      "loss = 20.687053\n",
      "accuracy = 0.849711\n",
      "loss = 20.685499\n",
      "accuracy = 0.849711\n",
      "loss = 20.678083\n",
      "accuracy = 0.849711\n",
      "loss = 20.676465\n",
      "accuracy = 0.849711\n",
      "loss = 20.669293\n",
      "accuracy = 0.849711\n",
      "loss = 20.672369\n",
      "accuracy = 0.849711\n",
      "loss = 20.664672\n",
      "accuracy = 0.849711\n",
      "loss = 20.657901\n",
      "accuracy = 0.849711\n",
      "loss = 20.660654\n",
      "accuracy = 0.849711\n",
      "loss = 20.653407\n",
      "accuracy = 0.849711\n",
      "loss = 20.650510\n",
      "accuracy = 0.849711\n",
      "loss = 20.647624\n",
      "accuracy = 0.843931\n",
      "loss = 20.644761\n",
      "accuracy = 0.849711\n",
      "loss = 20.637709\n",
      "accuracy = 0.849711\n",
      "loss = 20.634864\n",
      "accuracy = 0.849711\n",
      "loss = 20.632031\n",
      "accuracy = 0.849711\n",
      "loss = 20.629212\n",
      "accuracy = 0.849711\n",
      "loss = 20.626418\n",
      "accuracy = 0.849711\n",
      "loss = 20.619805\n",
      "accuracy = 0.849711\n",
      "loss = 20.622100\n",
      "accuracy = 0.849711\n",
      "loss = 20.619582\n",
      "accuracy = 0.849711\n",
      "loss = 20.622326\n",
      "accuracy = 0.849711\n",
      "loss = 20.615429\n",
      "accuracy = 0.849711\n",
      "loss = 20.617913\n",
      "accuracy = 0.849711\n",
      "loss = 20.611330\n",
      "accuracy = 0.849711\n",
      "loss = 20.613681\n",
      "accuracy = 0.849711\n",
      "loss = 20.611286\n",
      "accuracy = 0.849711\n",
      "loss = 20.609636\n",
      "accuracy = 0.849711\n",
      "loss = 20.612404\n",
      "accuracy = 0.855491\n",
      "loss = 20.605669\n",
      "accuracy = 0.855491\n",
      "loss = 20.608373\n",
      "accuracy = 0.855491\n",
      "loss = 20.605756\n",
      "accuracy = 0.855491\n",
      "loss = 20.603748\n",
      "accuracy = 0.855491\n",
      "loss = 20.606131\n",
      "accuracy = 0.855491\n",
      "loss = 20.603556\n",
      "accuracy = 0.855491\n",
      "loss = 20.601229\n",
      "accuracy = 0.855491\n",
      "loss = 20.604018\n",
      "accuracy = 0.855491\n",
      "loss = 20.601505\n",
      "accuracy = 0.855491\n",
      "loss = 20.594810\n",
      "accuracy = 0.855491\n",
      "loss = 20.596578\n",
      "accuracy = 0.855491\n",
      "loss = 20.594154\n",
      "accuracy = 0.855491\n",
      "loss = 20.596005\n",
      "accuracy = 0.855491\n",
      "loss = 20.589520\n",
      "accuracy = 0.855491\n",
      "loss = 20.596826\n",
      "accuracy = 0.855491\n",
      "loss = 20.589149\n",
      "accuracy = 0.855491\n",
      "loss = 20.591113\n",
      "accuracy = 0.855491\n",
      "loss = 20.580711\n",
      "accuracy = 0.855491\n",
      "loss = 20.586894\n",
      "accuracy = 0.855491\n",
      "loss = 20.580406\n",
      "accuracy = 0.855491\n",
      "loss = 20.587468\n",
      "accuracy = 0.855491\n",
      "loss = 20.576335\n",
      "accuracy = 0.855491\n",
      "loss = 20.578358\n",
      "accuracy = 0.855491\n",
      "loss = 20.580570\n",
      "accuracy = 0.855491\n",
      "loss = 20.577694\n",
      "accuracy = 0.855491\n",
      "loss = 20.580162\n",
      "accuracy = 0.855491\n",
      "loss = 20.572958\n",
      "accuracy = 0.849711\n",
      "loss = 20.575284\n",
      "accuracy = 0.849711\n",
      "loss = 20.572422\n",
      "accuracy = 0.849711\n",
      "loss = 20.574913\n",
      "accuracy = 0.849711\n",
      "loss = 20.572008\n",
      "accuracy = 0.849711\n",
      "loss = 20.569359\n",
      "accuracy = 0.849711\n",
      "loss = 20.571893\n",
      "accuracy = 0.849711\n",
      "loss = 20.564848\n",
      "accuracy = 0.849711\n",
      "loss = 20.567190\n",
      "accuracy = 0.849711\n",
      "loss = 20.564543\n",
      "accuracy = 0.849711\n",
      "loss = 20.567095\n",
      "accuracy = 0.849711\n",
      "loss = 20.560743\n",
      "accuracy = 0.849711\n",
      "loss = 20.563160\n",
      "accuracy = 0.849711\n",
      "loss = 20.561199\n",
      "accuracy = 0.849711\n",
      "loss = 20.558759\n",
      "accuracy = 0.849711\n",
      "loss = 20.561247\n",
      "accuracy = 0.849711\n",
      "loss = 20.558695\n",
      "accuracy = 0.849711\n",
      "loss = 20.561541\n",
      "accuracy = 0.849711\n",
      "loss = 20.554559\n",
      "accuracy = 0.849711\n",
      "loss = 20.557237\n",
      "accuracy = 0.849711\n",
      "loss = 20.551072\n",
      "accuracy = 0.849711\n",
      "loss = 20.553524\n",
      "accuracy = 0.849711\n",
      "loss = 20.556516\n",
      "accuracy = 0.849711\n",
      "loss = 20.549691\n",
      "accuracy = 0.849711\n",
      "loss = 20.553208\n",
      "accuracy = 0.849711\n",
      "loss = 20.552611\n",
      "accuracy = 0.849711\n",
      "loss = 20.552110\n",
      "accuracy = 0.849711\n",
      "loss = 20.556916\n",
      "accuracy = 0.849711\n",
      "loss = 20.552314\n",
      "accuracy = 0.849711\n",
      "loss = 20.556953\n",
      "accuracy = 0.849711\n",
      "loss = 20.556448\n",
      "accuracy = 0.849711\n",
      "loss = 20.555962\n",
      "accuracy = 0.849711\n",
      "loss = 20.555495\n",
      "accuracy = 0.849711\n",
      "loss = 20.555047\n",
      "accuracy = 0.849711\n",
      "loss = 20.554618\n",
      "accuracy = 0.849711\n",
      "loss = 20.554208\n",
      "accuracy = 0.849711\n",
      "loss = 20.553827\n",
      "accuracy = 0.849711\n",
      "loss = 20.558747\n",
      "accuracy = 0.849711\n",
      "loss = 20.558394\n",
      "accuracy = 0.849711\n",
      "loss = 20.558061\n",
      "accuracy = 0.849711\n",
      "loss = 20.557953\n",
      "accuracy = 0.849711\n",
      "loss = 20.556683\n",
      "accuracy = 0.849711\n",
      "loss = 20.556568\n",
      "accuracy = 0.849711\n",
      "loss = 20.560784\n",
      "accuracy = 0.849711\n",
      "loss = 20.559730\n",
      "accuracy = 0.849711\n",
      "loss = 20.565095\n",
      "accuracy = 0.849711\n",
      "loss = 20.563756\n",
      "accuracy = 0.849711\n",
      "loss = 20.562958\n",
      "accuracy = 0.849711\n",
      "loss = 20.568164\n",
      "accuracy = 0.849711\n",
      "loss = 20.566956\n",
      "accuracy = 0.849711\n",
      "loss = 20.567013\n",
      "accuracy = 0.849711\n",
      "loss = 20.566260\n",
      "accuracy = 0.849711\n",
      "loss = 20.571562\n",
      "accuracy = 0.849711\n",
      "loss = 20.570413\n",
      "accuracy = 0.849711\n",
      "loss = 20.574761\n",
      "accuracy = 0.849711\n",
      "loss = 20.573949\n",
      "accuracy = 0.849711\n",
      "loss = 20.579440\n",
      "accuracy = 0.849711\n",
      "loss = 20.578293\n",
      "accuracy = 0.849711\n",
      "loss = 20.578580\n",
      "accuracy = 0.849711\n",
      "loss = 20.572257\n",
      "accuracy = 0.849711\n",
      "loss = 20.577283\n",
      "accuracy = 0.849711\n",
      "loss = 20.582075\n",
      "accuracy = 0.849711\n",
      "loss = 20.575293\n",
      "accuracy = 0.849711\n",
      "loss = 20.580755\n",
      "accuracy = 0.849711\n",
      "loss = 20.574188\n",
      "accuracy = 0.849711\n",
      "loss = 20.579514\n",
      "accuracy = 0.849711\n",
      "loss = 20.573155\n",
      "accuracy = 0.849711\n",
      "loss = 20.578521\n",
      "accuracy = 0.849711\n",
      "loss = 20.583409\n",
      "accuracy = 0.849711\n",
      "loss = 20.576569\n",
      "accuracy = 0.849711\n",
      "loss = 20.581012\n",
      "accuracy = 0.849711\n",
      "loss = 20.580736\n",
      "accuracy = 0.849711\n",
      "loss = 20.579391\n",
      "accuracy = 0.849711\n",
      "loss = 20.584266\n",
      "accuracy = 0.849711\n",
      "loss = 20.578106\n",
      "accuracy = 0.849711\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAoQxxpiQLEAYY4wJ6f8BdySxTXXIn+kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e012e18438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def loadDataSet():\n",
    "\t# 读取数据\n",
    "\tX,y=load_svmlight_file(\"australian_scale.txt\")\n",
    "\t#将稀疏矩阵转化为完整特征矩阵\n",
    "\tX = X.todense()\n",
    "\t# 将数据集切分为训练集和验证集 \n",
    "\tX_train, X_validation, y_train, y_validation = train_test_split(X, y, random_state=0)\n",
    "\tprint(X_train.shape,y_train.shape)\n",
    "\treturn X_train, X_validation, y_train, y_validation\n",
    "\n",
    "def gradDescent(alpha,maxCycles,X_data,y_data):\n",
    "\tnum = y_data.shape[0]    #样本数量\n",
    "\t# 线性模型参数正态分布初始化\n",
    "\tw = np.random.normal(size=(X_data.shape[1]))\n",
    "\tb = np.random.normal(size=1)\n",
    "\tlosss = []\n",
    "\ttv = 0\n",
    "\n",
    "\t#迭代次maxCycles次\n",
    "\tfor n in range(maxCycles):\n",
    "\t\tgrad_w = np.ones(X_data.shape[1])*(np.linalg.norm(w,ord=2))\n",
    "\t\tgrad_b = np.zeros(1)\n",
    "\t\tloss = np.power(np.linalg.norm(w,ord=2),2) / 2    #求w的第二范式\n",
    "\t\terror = 0\n",
    "\t\tC = 1/np.power(2,2)\n",
    "\t\tfor i in range(num):\n",
    "\t\t\ty = np.dot( X_data[i][0].getA()[0], w ) + b\n",
    "\t\t\tif y_data[i] * y < 1:\n",
    "\t\t\t\tloss += C * max(0,1 - y_data[i] * y) \n",
    "\t\t\t\tgrad_w += - C * y_data[i] * X_data[i][0].getA()[0] \n",
    "\t\t\t\tgrad_b += - C * y_data[i] \n",
    "\t\t\tif y > tv :\n",
    "\t\t\t\ty = 1\n",
    "\t\t\telse: y = -1\n",
    "\t\t\tif not y == y_data[i]:\n",
    "\t\t\t\terror += 1\n",
    "\t\t#更新模型参数\n",
    "\t\tw -= alpha * grad_w\n",
    "\t\tb -= alpha * grad_b\n",
    "\t\tlosss.append(loss)\n",
    "\t\tprint(\"loss = %f\" % loss)\n",
    "\t\tprint(\"accuracy = %f\" % (1-error/num))\n",
    "\treturn losss\n",
    "\n",
    "def plotLossPerTime(n,losss_train,losss_validation):\n",
    "\tplt.xlabel('iteration times')\n",
    "\tplt.ylabel('loss of train or validation')\n",
    "\tplt.title('linear classification & gradient decrease')\n",
    "\tn_cycles = range(1,n+1)\n",
    "\tplt.plot(n_cycles, losss_train, label = \"Loss of Train\", color='blue', linewidth=3)\n",
    "\tplt.plot(n_cycles, losss_validation, label = \"Loss of Validation\", color='red', linewidth=3)\n",
    "\tplt.legend(loc=0)\n",
    "\tplt.grid()\n",
    "\tplt.show()\n",
    "\n",
    "# main\n",
    "X_train, X_validation, y_train, y_validation = loadDataSet()\n",
    "alpha = 0.001\n",
    "maxCycles = 500\n",
    "losss_train = gradDescent(alpha,maxCycles,X_train,y_train)\n",
    "losss_validation = gradDescent(alpha,maxCycles,X_validation,y_validation)\n",
    "plotLossPerTime(maxCycles,losss_train,losss_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
