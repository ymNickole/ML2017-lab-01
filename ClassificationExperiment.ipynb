{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(517, 14) (517,)\n",
      "loss = 278.245932\n",
      "accuracy = 0.500967\n",
      "loss = 266.928267\n",
      "accuracy = 0.499033\n",
      "loss = 255.743697\n",
      "accuracy = 0.499033\n",
      "loss = 244.798975\n",
      "accuracy = 0.500967\n",
      "loss = 234.111507\n",
      "accuracy = 0.502901\n",
      "loss = 223.888368\n",
      "accuracy = 0.493230\n",
      "loss = 213.982739\n",
      "accuracy = 0.491296\n",
      "loss = 204.341863\n",
      "accuracy = 0.500967\n",
      "loss = 194.947195\n",
      "accuracy = 0.500967\n",
      "loss = 185.854933\n",
      "accuracy = 0.497099\n",
      "loss = 177.076840\n",
      "accuracy = 0.508704\n",
      "loss = 168.428926\n",
      "accuracy = 0.518375\n",
      "loss = 160.136144\n",
      "accuracy = 0.522244\n",
      "loss = 152.355237\n",
      "accuracy = 0.529981\n",
      "loss = 144.765094\n",
      "accuracy = 0.549323\n",
      "loss = 137.557039\n",
      "accuracy = 0.555126\n",
      "loss = 130.630661\n",
      "accuracy = 0.578337\n",
      "loss = 124.146125\n",
      "accuracy = 0.597679\n",
      "loss = 118.110204\n",
      "accuracy = 0.628627\n",
      "loss = 112.570054\n",
      "accuracy = 0.649903\n",
      "loss = 107.551419\n",
      "accuracy = 0.667311\n",
      "loss = 103.025436\n",
      "accuracy = 0.680851\n",
      "loss = 98.798252\n",
      "accuracy = 0.698259\n",
      "loss = 94.782001\n",
      "accuracy = 0.711799\n",
      "loss = 91.122485\n",
      "accuracy = 0.723404\n",
      "loss = 87.953750\n",
      "accuracy = 0.727273\n",
      "loss = 85.298580\n",
      "accuracy = 0.735010\n",
      "loss = 82.961509\n",
      "accuracy = 0.742747\n",
      "loss = 80.924758\n",
      "accuracy = 0.748549\n",
      "loss = 79.025756\n",
      "accuracy = 0.754352\n",
      "loss = 77.186289\n",
      "accuracy = 0.760155\n",
      "loss = 75.528029\n",
      "accuracy = 0.764023\n",
      "loss = 74.134932\n",
      "accuracy = 0.773694\n",
      "loss = 72.976142\n",
      "accuracy = 0.773694\n",
      "loss = 72.059880\n",
      "accuracy = 0.777563\n",
      "loss = 71.270399\n",
      "accuracy = 0.779497\n",
      "loss = 70.558010\n",
      "accuracy = 0.783366\n",
      "loss = 69.912131\n",
      "accuracy = 0.785300\n",
      "loss = 69.317856\n",
      "accuracy = 0.796905\n",
      "loss = 68.745888\n",
      "accuracy = 0.800774\n",
      "loss = 68.212102\n",
      "accuracy = 0.808511\n",
      "loss = 67.703439\n",
      "accuracy = 0.810445\n",
      "loss = 67.261393\n",
      "accuracy = 0.810445\n",
      "loss = 66.877916\n",
      "accuracy = 0.810445\n",
      "loss = 66.523925\n",
      "accuracy = 0.810445\n",
      "loss = 66.206337\n",
      "accuracy = 0.810445\n",
      "loss = 65.915785\n",
      "accuracy = 0.810445\n",
      "loss = 65.629840\n",
      "accuracy = 0.808511\n",
      "loss = 65.364053\n",
      "accuracy = 0.812379\n",
      "loss = 65.098331\n",
      "accuracy = 0.814313\n",
      "loss = 64.852800\n",
      "accuracy = 0.816248\n",
      "loss = 64.616567\n",
      "accuracy = 0.816248\n",
      "loss = 64.388079\n",
      "accuracy = 0.816248\n",
      "loss = 64.184982\n",
      "accuracy = 0.818182\n",
      "loss = 63.997974\n",
      "accuracy = 0.820116\n",
      "loss = 63.820748\n",
      "accuracy = 0.822050\n",
      "loss = 63.667062\n",
      "accuracy = 0.823985\n",
      "loss = 63.513428\n",
      "accuracy = 0.823985\n",
      "loss = 63.367514\n",
      "accuracy = 0.823985\n",
      "loss = 63.224678\n",
      "accuracy = 0.823985\n",
      "loss = 63.095296\n",
      "accuracy = 0.823985\n",
      "loss = 62.968094\n",
      "accuracy = 0.825919\n",
      "loss = 62.840914\n",
      "accuracy = 0.825919\n",
      "loss = 62.723936\n",
      "accuracy = 0.825919\n",
      "loss = 62.611870\n",
      "accuracy = 0.827853\n",
      "loss = 62.510317\n",
      "accuracy = 0.829787\n",
      "loss = 62.408515\n",
      "accuracy = 0.829787\n",
      "loss = 62.306958\n",
      "accuracy = 0.827853\n",
      "loss = 62.217485\n",
      "accuracy = 0.825919\n",
      "loss = 62.126588\n",
      "accuracy = 0.825919\n",
      "loss = 62.035448\n",
      "accuracy = 0.825919\n",
      "loss = 61.945541\n",
      "accuracy = 0.825919\n",
      "loss = 61.856838\n",
      "accuracy = 0.825919\n",
      "loss = 61.776499\n",
      "accuracy = 0.827853\n",
      "loss = 61.696342\n",
      "accuracy = 0.827853\n",
      "loss = 61.617579\n",
      "accuracy = 0.827853\n",
      "loss = 61.538813\n",
      "accuracy = 0.827853\n",
      "loss = 61.458534\n",
      "accuracy = 0.827853\n",
      "loss = 61.378501\n",
      "accuracy = 0.827853\n",
      "loss = 61.308656\n",
      "accuracy = 0.829787\n",
      "loss = 61.237147\n",
      "accuracy = 0.829787\n",
      "loss = 61.167726\n",
      "accuracy = 0.829787\n",
      "loss = 61.098065\n",
      "accuracy = 0.833656\n",
      "loss = 61.028163\n",
      "accuracy = 0.833656\n",
      "loss = 60.958022\n",
      "accuracy = 0.833656\n",
      "loss = 60.887642\n",
      "accuracy = 0.833656\n",
      "loss = 60.818625\n",
      "accuracy = 0.837524\n",
      "loss = 60.750084\n",
      "accuracy = 0.837524\n",
      "loss = 60.685045\n",
      "accuracy = 0.837524\n",
      "loss = 60.629329\n",
      "accuracy = 0.837524\n",
      "loss = 60.573375\n",
      "accuracy = 0.837524\n",
      "loss = 60.517184\n",
      "accuracy = 0.837524\n",
      "loss = 60.460758\n",
      "accuracy = 0.837524\n",
      "loss = 60.404119\n",
      "accuracy = 0.837524\n",
      "loss = 60.348495\n",
      "accuracy = 0.839458\n",
      "loss = 60.299639\n",
      "accuracy = 0.841393\n",
      "loss = 60.249905\n",
      "accuracy = 0.841393\n",
      "loss = 60.208958\n",
      "accuracy = 0.841393\n",
      "loss = 60.167773\n",
      "accuracy = 0.843327\n",
      "loss = 60.126348\n",
      "accuracy = 0.843327\n",
      "loss = 60.084685\n",
      "accuracy = 0.843327\n",
      "loss = 60.042784\n",
      "accuracy = 0.843327\n",
      "loss = 60.000646\n",
      "accuracy = 0.843327\n",
      "loss = 59.958271\n",
      "accuracy = 0.843327\n",
      "loss = 59.915660\n",
      "accuracy = 0.843327\n",
      "loss = 59.872813\n",
      "accuracy = 0.843327\n",
      "loss = 59.829730\n",
      "accuracy = 0.843327\n",
      "loss = 59.786413\n",
      "accuracy = 0.843327\n",
      "loss = 59.742860\n",
      "accuracy = 0.843327\n",
      "loss = 59.699074\n",
      "accuracy = 0.843327\n",
      "loss = 59.655054\n",
      "accuracy = 0.843327\n",
      "loss = 59.610801\n",
      "accuracy = 0.843327\n",
      "loss = 59.566315\n",
      "accuracy = 0.843327\n",
      "loss = 59.521598\n",
      "accuracy = 0.843327\n",
      "loss = 59.476648\n",
      "accuracy = 0.843327\n",
      "loss = 59.431467\n",
      "accuracy = 0.843327\n",
      "loss = 59.386056\n",
      "accuracy = 0.843327\n",
      "loss = 59.340414\n",
      "accuracy = 0.843327\n",
      "loss = 59.295020\n",
      "accuracy = 0.843327\n",
      "loss = 59.255430\n",
      "accuracy = 0.843327\n",
      "loss = 59.218718\n",
      "accuracy = 0.845261\n",
      "loss = 59.178773\n",
      "accuracy = 0.845261\n",
      "loss = 59.135249\n",
      "accuracy = 0.845261\n",
      "loss = 59.094871\n",
      "accuracy = 0.845261\n",
      "loss = 59.053736\n",
      "accuracy = 0.845261\n",
      "loss = 59.008428\n",
      "accuracy = 0.845261\n",
      "loss = 58.962888\n",
      "accuracy = 0.845261\n",
      "loss = 58.917285\n",
      "accuracy = 0.845261\n",
      "loss = 58.881840\n",
      "accuracy = 0.845261\n",
      "loss = 58.846170\n",
      "accuracy = 0.845261\n",
      "loss = 58.810275\n",
      "accuracy = 0.845261\n",
      "loss = 58.774154\n",
      "accuracy = 0.845261\n",
      "loss = 58.737810\n",
      "accuracy = 0.845261\n",
      "loss = 58.701241\n",
      "accuracy = 0.845261\n",
      "loss = 58.664449\n",
      "accuracy = 0.845261\n",
      "loss = 58.628082\n",
      "accuracy = 0.845261\n",
      "loss = 58.595885\n",
      "accuracy = 0.845261\n",
      "loss = 58.556736\n",
      "accuracy = 0.845261\n",
      "loss = 58.522751\n",
      "accuracy = 0.845261\n",
      "loss = 58.488541\n",
      "accuracy = 0.845261\n",
      "loss = 58.454118\n",
      "accuracy = 0.845261\n",
      "loss = 58.413335\n",
      "accuracy = 0.845261\n",
      "loss = 58.378450\n",
      "accuracy = 0.845261\n",
      "loss = 58.343625\n",
      "accuracy = 0.847195\n",
      "loss = 58.301898\n",
      "accuracy = 0.847195\n",
      "loss = 58.266570\n",
      "accuracy = 0.847195\n",
      "loss = 58.224451\n",
      "accuracy = 0.847195\n",
      "loss = 58.188597\n",
      "accuracy = 0.847195\n",
      "loss = 58.146111\n",
      "accuracy = 0.847195\n",
      "loss = 58.109709\n",
      "accuracy = 0.847195\n",
      "loss = 58.066883\n",
      "accuracy = 0.847195\n",
      "loss = 58.029992\n",
      "accuracy = 0.847195\n",
      "loss = 57.993969\n",
      "accuracy = 0.847195\n",
      "loss = 57.933996\n",
      "accuracy = 0.847195\n",
      "loss = 57.893248\n",
      "accuracy = 0.847195\n",
      "loss = 57.852275\n",
      "accuracy = 0.847195\n",
      "loss = 57.811076\n",
      "accuracy = 0.849130\n",
      "loss = 57.769653\n",
      "accuracy = 0.849130\n",
      "loss = 57.728005\n",
      "accuracy = 0.849130\n",
      "loss = 57.686133\n",
      "accuracy = 0.849130\n",
      "loss = 57.644038\n",
      "accuracy = 0.849130\n",
      "loss = 57.601719\n",
      "accuracy = 0.849130\n",
      "loss = 57.559177\n",
      "accuracy = 0.849130\n",
      "loss = 57.516413\n",
      "accuracy = 0.849130\n",
      "loss = 57.473719\n",
      "accuracy = 0.849130\n",
      "loss = 57.433082\n",
      "accuracy = 0.849130\n",
      "loss = 57.392321\n",
      "accuracy = 0.849130\n",
      "loss = 57.349887\n",
      "accuracy = 0.849130\n",
      "loss = 57.317790\n",
      "accuracy = 0.849130\n",
      "loss = 57.276346\n",
      "accuracy = 0.849130\n",
      "loss = 57.242192\n",
      "accuracy = 0.849130\n",
      "loss = 57.206934\n",
      "accuracy = 0.849130\n",
      "loss = 57.171566\n",
      "accuracy = 0.849130\n",
      "loss = 57.136836\n",
      "accuracy = 0.849130\n",
      "loss = 57.100917\n",
      "accuracy = 0.849130\n",
      "loss = 57.064931\n",
      "accuracy = 0.849130\n",
      "loss = 57.034922\n",
      "accuracy = 0.849130\n",
      "loss = 56.984879\n",
      "accuracy = 0.849130\n",
      "loss = 56.952626\n",
      "accuracy = 0.849130\n",
      "loss = 56.907379\n",
      "accuracy = 0.849130\n",
      "loss = 56.873512\n",
      "accuracy = 0.851064\n",
      "loss = 56.820352\n",
      "accuracy = 0.851064\n",
      "loss = 56.788060\n",
      "accuracy = 0.851064\n",
      "loss = 56.744890\n",
      "accuracy = 0.851064\n",
      "loss = 56.709219\n",
      "accuracy = 0.851064\n",
      "loss = 56.676050\n",
      "accuracy = 0.851064\n",
      "loss = 56.614925\n",
      "accuracy = 0.851064\n",
      "loss = 56.582044\n",
      "accuracy = 0.851064\n",
      "loss = 56.540412\n",
      "accuracy = 0.851064\n",
      "loss = 56.504412\n",
      "accuracy = 0.851064\n",
      "loss = 56.461044\n",
      "accuracy = 0.852998\n",
      "loss = 56.421211\n",
      "accuracy = 0.852998\n",
      "loss = 56.381365\n",
      "accuracy = 0.852998\n",
      "loss = 56.338201\n",
      "accuracy = 0.852998\n",
      "loss = 56.298010\n",
      "accuracy = 0.852998\n",
      "loss = 56.255203\n",
      "accuracy = 0.852998\n",
      "loss = 56.217542\n",
      "accuracy = 0.852998\n",
      "loss = 56.179674\n",
      "accuracy = 0.852998\n",
      "loss = 56.141598\n",
      "accuracy = 0.852998\n",
      "loss = 56.103425\n",
      "accuracy = 0.852998\n",
      "loss = 56.057380\n",
      "accuracy = 0.852998\n",
      "loss = 56.018686\n",
      "accuracy = 0.852998\n",
      "loss = 55.979971\n",
      "accuracy = 0.852998\n",
      "loss = 55.933253\n",
      "accuracy = 0.852998\n",
      "loss = 55.892544\n",
      "accuracy = 0.852998\n",
      "loss = 55.854436\n",
      "accuracy = 0.852998\n",
      "loss = 55.813265\n",
      "accuracy = 0.852998\n",
      "loss = 55.774806\n",
      "accuracy = 0.852998\n",
      "loss = 55.733186\n",
      "accuracy = 0.852998\n",
      "loss = 55.694366\n",
      "accuracy = 0.852998\n",
      "loss = 55.652580\n",
      "accuracy = 0.852998\n",
      "loss = 55.614893\n",
      "accuracy = 0.852998\n",
      "loss = 55.572826\n",
      "accuracy = 0.852998\n",
      "loss = 55.531737\n",
      "accuracy = 0.852998\n",
      "loss = 55.490539\n",
      "accuracy = 0.852998\n",
      "loss = 55.452061\n",
      "accuracy = 0.852998\n",
      "loss = 55.410428\n",
      "accuracy = 0.852998\n",
      "loss = 55.367346\n",
      "accuracy = 0.852998\n",
      "loss = 55.325374\n",
      "accuracy = 0.851064\n",
      "loss = 55.286089\n",
      "accuracy = 0.851064\n",
      "loss = 55.243619\n",
      "accuracy = 0.851064\n",
      "loss = 55.200997\n",
      "accuracy = 0.852998\n",
      "loss = 55.157102\n",
      "accuracy = 0.852998\n",
      "loss = 55.117180\n",
      "accuracy = 0.852998\n",
      "loss = 55.084732\n",
      "accuracy = 0.852998\n",
      "loss = 55.051077\n",
      "accuracy = 0.852998\n",
      "loss = 55.011138\n",
      "accuracy = 0.852998\n",
      "loss = 54.978382\n",
      "accuracy = 0.852998\n",
      "loss = 54.934135\n",
      "accuracy = 0.852998\n",
      "loss = 54.901353\n",
      "accuracy = 0.852998\n",
      "loss = 54.856549\n",
      "accuracy = 0.852998\n",
      "loss = 54.819180\n",
      "accuracy = 0.852998\n",
      "loss = 54.777743\n",
      "accuracy = 0.852998\n",
      "loss = 54.741300\n",
      "accuracy = 0.852998\n",
      "loss = 54.716331\n",
      "accuracy = 0.852998\n",
      "loss = 54.671512\n",
      "accuracy = 0.852998\n",
      "loss = 54.641832\n",
      "accuracy = 0.852998\n",
      "loss = 54.601385\n",
      "accuracy = 0.852998\n",
      "loss = 54.564896\n",
      "accuracy = 0.854932\n",
      "loss = 54.536093\n",
      "accuracy = 0.852998\n",
      "loss = 54.495260\n",
      "accuracy = 0.854932\n",
      "loss = 54.458380\n",
      "accuracy = 0.854932\n",
      "loss = 54.428536\n",
      "accuracy = 0.854932\n",
      "loss = 54.395454\n",
      "accuracy = 0.854932\n",
      "loss = 54.362191\n",
      "accuracy = 0.854932\n",
      "loss = 54.328745\n",
      "accuracy = 0.854932\n",
      "loss = 54.295221\n",
      "accuracy = 0.854932\n",
      "loss = 54.264368\n",
      "accuracy = 0.854932\n",
      "loss = 54.230381\n",
      "accuracy = 0.854932\n",
      "loss = 54.196213\n",
      "accuracy = 0.854932\n",
      "loss = 54.161976\n",
      "accuracy = 0.854932\n",
      "loss = 54.130709\n",
      "accuracy = 0.854932\n",
      "loss = 54.087967\n",
      "accuracy = 0.854932\n",
      "loss = 54.055690\n",
      "accuracy = 0.854932\n",
      "loss = 54.023653\n",
      "accuracy = 0.854932\n",
      "loss = 53.984485\n",
      "accuracy = 0.854932\n",
      "loss = 53.951864\n",
      "accuracy = 0.854932\n",
      "loss = 53.916259\n",
      "accuracy = 0.854932\n",
      "loss = 53.880704\n",
      "accuracy = 0.854932\n",
      "loss = 53.845517\n",
      "accuracy = 0.854932\n",
      "loss = 53.809496\n",
      "accuracy = 0.854932\n",
      "loss = 53.776164\n",
      "accuracy = 0.854932\n",
      "loss = 53.737644\n",
      "accuracy = 0.854932\n",
      "loss = 53.703953\n",
      "accuracy = 0.854932\n",
      "loss = 53.667104\n",
      "accuracy = 0.854932\n",
      "loss = 53.630128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.854932\n",
      "loss = 53.596091\n",
      "accuracy = 0.854932\n",
      "loss = 53.556557\n",
      "accuracy = 0.854932\n",
      "loss = 53.524477\n",
      "accuracy = 0.854932\n",
      "loss = 53.480695\n",
      "accuracy = 0.854932\n",
      "loss = 53.447784\n",
      "accuracy = 0.854932\n",
      "loss = 53.409706\n",
      "accuracy = 0.854932\n",
      "loss = 53.371747\n",
      "accuracy = 0.854932\n",
      "loss = 53.335467\n",
      "accuracy = 0.854932\n",
      "loss = 53.297067\n",
      "accuracy = 0.854932\n",
      "loss = 53.264868\n",
      "accuracy = 0.854932\n",
      "loss = 53.220779\n",
      "accuracy = 0.854932\n",
      "loss = 53.187502\n",
      "accuracy = 0.854932\n",
      "loss = 53.152488\n",
      "accuracy = 0.854932\n",
      "loss = 53.117345\n",
      "accuracy = 0.854932\n",
      "loss = 53.078118\n",
      "accuracy = 0.854932\n",
      "loss = 53.039148\n",
      "accuracy = 0.854932\n",
      "loss = 53.013698\n",
      "accuracy = 0.854932\n",
      "loss = 52.969819\n",
      "accuracy = 0.854932\n",
      "loss = 52.941601\n",
      "accuracy = 0.854932\n",
      "loss = 52.898304\n",
      "accuracy = 0.854932\n",
      "loss = 52.871635\n",
      "accuracy = 0.854932\n",
      "loss = 52.835173\n",
      "accuracy = 0.854932\n",
      "loss = 52.804455\n",
      "accuracy = 0.854932\n",
      "loss = 52.767774\n",
      "accuracy = 0.854932\n",
      "loss = 52.737109\n",
      "accuracy = 0.854932\n",
      "loss = 52.688908\n",
      "accuracy = 0.854932\n",
      "loss = 52.657996\n",
      "accuracy = 0.854932\n",
      "loss = 52.615285\n",
      "accuracy = 0.854932\n",
      "loss = 52.588225\n",
      "accuracy = 0.854932\n",
      "loss = 52.544869\n",
      "accuracy = 0.854932\n",
      "loss = 52.507920\n",
      "accuracy = 0.854932\n",
      "loss = 52.471497\n",
      "accuracy = 0.854932\n",
      "loss = 52.439054\n",
      "accuracy = 0.854932\n",
      "loss = 52.396634\n",
      "accuracy = 0.854932\n",
      "loss = 52.363475\n",
      "accuracy = 0.854932\n",
      "loss = 52.330157\n",
      "accuracy = 0.854932\n",
      "loss = 52.296984\n",
      "accuracy = 0.854932\n",
      "loss = 52.257696\n",
      "accuracy = 0.854932\n",
      "loss = 52.223897\n",
      "accuracy = 0.854932\n",
      "loss = 52.190188\n",
      "accuracy = 0.854932\n",
      "loss = 52.154136\n",
      "accuracy = 0.856867\n",
      "loss = 52.120296\n",
      "accuracy = 0.856867\n",
      "loss = 52.080472\n",
      "accuracy = 0.856867\n",
      "loss = 52.054044\n",
      "accuracy = 0.856867\n",
      "loss = 52.017390\n",
      "accuracy = 0.856867\n",
      "loss = 51.982848\n",
      "accuracy = 0.856867\n",
      "loss = 51.942617\n",
      "accuracy = 0.856867\n",
      "loss = 51.915311\n",
      "accuracy = 0.856867\n",
      "loss = 51.878279\n",
      "accuracy = 0.856867\n",
      "loss = 51.851704\n",
      "accuracy = 0.856867\n",
      "loss = 51.806540\n",
      "accuracy = 0.856867\n",
      "loss = 51.778642\n",
      "accuracy = 0.856867\n",
      "loss = 51.739161\n",
      "accuracy = 0.856867\n",
      "loss = 51.711691\n",
      "accuracy = 0.856867\n",
      "loss = 51.669676\n",
      "accuracy = 0.856867\n",
      "loss = 51.641470\n",
      "accuracy = 0.856867\n",
      "loss = 51.599583\n",
      "accuracy = 0.856867\n",
      "loss = 51.570720\n",
      "accuracy = 0.856867\n",
      "loss = 51.532201\n",
      "accuracy = 0.856867\n",
      "loss = 51.503689\n",
      "accuracy = 0.856867\n",
      "loss = 51.460905\n",
      "accuracy = 0.856867\n",
      "loss = 51.431518\n",
      "accuracy = 0.856867\n",
      "loss = 51.392323\n",
      "accuracy = 0.856867\n",
      "loss = 51.363311\n",
      "accuracy = 0.856867\n",
      "loss = 51.319830\n",
      "accuracy = 0.856867\n",
      "loss = 51.289942\n",
      "accuracy = 0.856867\n",
      "loss = 51.249880\n",
      "accuracy = 0.856867\n",
      "loss = 51.223320\n",
      "accuracy = 0.856867\n",
      "loss = 51.176448\n",
      "accuracy = 0.856867\n",
      "loss = 51.148495\n",
      "accuracy = 0.856867\n",
      "loss = 51.115877\n",
      "accuracy = 0.856867\n",
      "loss = 51.076682\n",
      "accuracy = 0.856867\n",
      "loss = 51.043736\n",
      "accuracy = 0.856867\n",
      "loss = 51.001683\n",
      "accuracy = 0.856867\n",
      "loss = 50.978635\n",
      "accuracy = 0.856867\n",
      "loss = 50.939275\n",
      "accuracy = 0.856867\n",
      "loss = 50.918592\n",
      "accuracy = 0.856867\n",
      "loss = 50.887403\n",
      "accuracy = 0.856867\n",
      "loss = 50.850230\n",
      "accuracy = 0.856867\n",
      "loss = 50.830979\n",
      "accuracy = 0.856867\n",
      "loss = 50.792846\n",
      "accuracy = 0.856867\n",
      "loss = 50.758990\n",
      "accuracy = 0.856867\n",
      "loss = 50.735464\n",
      "accuracy = 0.856867\n",
      "loss = 50.697872\n",
      "accuracy = 0.856867\n",
      "loss = 50.680230\n",
      "accuracy = 0.856867\n",
      "loss = 50.640317\n",
      "accuracy = 0.856867\n",
      "loss = 50.604389\n",
      "accuracy = 0.856867\n",
      "loss = 50.579660\n",
      "accuracy = 0.856867\n",
      "loss = 50.544009\n",
      "accuracy = 0.856867\n",
      "loss = 50.524175\n",
      "accuracy = 0.856867\n",
      "loss = 50.492386\n",
      "accuracy = 0.856867\n",
      "loss = 50.472172\n",
      "accuracy = 0.856867\n",
      "loss = 50.444154\n",
      "accuracy = 0.856867\n",
      "loss = 50.419707\n",
      "accuracy = 0.856867\n",
      "loss = 50.387745\n",
      "accuracy = 0.856867\n",
      "loss = 50.362748\n",
      "accuracy = 0.856867\n",
      "loss = 50.337832\n",
      "accuracy = 0.856867\n",
      "loss = 50.308879\n",
      "accuracy = 0.856867\n",
      "loss = 50.283846\n",
      "accuracy = 0.856867\n",
      "loss = 50.251107\n",
      "accuracy = 0.856867\n",
      "loss = 50.225430\n",
      "accuracy = 0.854932\n",
      "loss = 50.199776\n",
      "accuracy = 0.854932\n",
      "loss = 50.167003\n",
      "accuracy = 0.854932\n",
      "loss = 50.148604\n",
      "accuracy = 0.854932\n",
      "loss = 50.118256\n",
      "accuracy = 0.854932\n",
      "loss = 50.091896\n",
      "accuracy = 0.854932\n",
      "loss = 50.065569\n",
      "accuracy = 0.854932\n",
      "loss = 50.032633\n",
      "accuracy = 0.854932\n",
      "loss = 50.016626\n",
      "accuracy = 0.854932\n",
      "loss = 49.982079\n",
      "accuracy = 0.854932\n",
      "loss = 49.965527\n",
      "accuracy = 0.854932\n",
      "loss = 49.931014\n",
      "accuracy = 0.854932\n",
      "loss = 49.914051\n",
      "accuracy = 0.854932\n",
      "loss = 49.879673\n",
      "accuracy = 0.856867\n",
      "loss = 49.858957\n",
      "accuracy = 0.856867\n",
      "loss = 49.831325\n",
      "accuracy = 0.856867\n",
      "loss = 49.811249\n",
      "accuracy = 0.856867\n",
      "loss = 49.775837\n",
      "accuracy = 0.854932\n",
      "loss = 49.757857\n",
      "accuracy = 0.854932\n",
      "loss = 49.722730\n",
      "accuracy = 0.854932\n",
      "loss = 49.704147\n",
      "accuracy = 0.854932\n",
      "loss = 49.679382\n",
      "accuracy = 0.854932\n",
      "loss = 49.641721\n",
      "accuracy = 0.854932\n",
      "loss = 49.622755\n",
      "accuracy = 0.854932\n",
      "loss = 49.594292\n",
      "accuracy = 0.854932\n",
      "loss = 49.572660\n",
      "accuracy = 0.854932\n",
      "loss = 49.543028\n",
      "accuracy = 0.854932\n",
      "loss = 49.517935\n",
      "accuracy = 0.854932\n",
      "loss = 49.492907\n",
      "accuracy = 0.854932\n",
      "loss = 49.460983\n",
      "accuracy = 0.854932\n",
      "loss = 49.443535\n",
      "accuracy = 0.854932\n",
      "loss = 49.410351\n",
      "accuracy = 0.854932\n",
      "loss = 49.381369\n",
      "accuracy = 0.854932\n",
      "loss = 49.355496\n",
      "accuracy = 0.856867\n",
      "loss = 49.325918\n",
      "accuracy = 0.856867\n",
      "loss = 49.299967\n",
      "accuracy = 0.858801\n",
      "loss = 49.266910\n",
      "accuracy = 0.858801\n",
      "loss = 49.240412\n",
      "accuracy = 0.858801\n",
      "loss = 49.213853\n",
      "accuracy = 0.858801\n",
      "loss = 49.180640\n",
      "accuracy = 0.858801\n",
      "loss = 49.153761\n",
      "accuracy = 0.858801\n",
      "loss = 49.126758\n",
      "accuracy = 0.858801\n",
      "loss = 49.099896\n",
      "accuracy = 0.858801\n",
      "loss = 49.065979\n",
      "accuracy = 0.858801\n",
      "loss = 49.038597\n",
      "accuracy = 0.858801\n",
      "loss = 49.011134\n",
      "accuracy = 0.858801\n",
      "loss = 48.980111\n",
      "accuracy = 0.858801\n",
      "loss = 48.952383\n",
      "accuracy = 0.858801\n",
      "loss = 48.921431\n",
      "accuracy = 0.858801\n",
      "loss = 48.893538\n",
      "accuracy = 0.858801\n",
      "loss = 48.862420\n",
      "accuracy = 0.858801\n",
      "loss = 48.850773\n",
      "accuracy = 0.858801\n",
      "loss = 48.814700\n",
      "accuracy = 0.858801\n",
      "loss = 48.782963\n",
      "accuracy = 0.858801\n",
      "loss = 48.762981\n",
      "accuracy = 0.858801\n",
      "loss = 48.727546\n",
      "accuracy = 0.858801\n",
      "loss = 48.706826\n",
      "accuracy = 0.858801\n",
      "loss = 48.672473\n",
      "accuracy = 0.858801\n",
      "loss = 48.662959\n",
      "accuracy = 0.858801\n",
      "loss = 48.626920\n",
      "accuracy = 0.858801\n",
      "loss = 48.596697\n",
      "accuracy = 0.858801\n",
      "loss = 48.582712\n",
      "accuracy = 0.858801\n",
      "loss = 48.550870\n",
      "accuracy = 0.858801\n",
      "loss = 48.528490\n",
      "accuracy = 0.858801\n",
      "loss = 48.503916\n",
      "accuracy = 0.858801\n",
      "loss = 48.483251\n",
      "accuracy = 0.856867\n",
      "loss = 48.459303\n",
      "accuracy = 0.854932\n",
      "loss = 48.438892\n",
      "accuracy = 0.854932\n",
      "loss = 48.413536\n",
      "accuracy = 0.854932\n",
      "loss = 48.392796\n",
      "accuracy = 0.854932\n",
      "loss = 48.366317\n",
      "accuracy = 0.854932\n",
      "loss = 48.352706\n",
      "accuracy = 0.856867\n",
      "loss = 48.326778\n",
      "accuracy = 0.856867\n",
      "loss = 48.305526\n",
      "accuracy = 0.856867\n",
      "loss = 48.275536\n",
      "accuracy = 0.856867\n",
      "loss = 48.252419\n",
      "accuracy = 0.856867\n",
      "loss = 48.229182\n",
      "accuracy = 0.856867\n",
      "loss = 48.205951\n",
      "accuracy = 0.856867\n",
      "loss = 48.176110\n",
      "accuracy = 0.856867\n",
      "loss = 48.168293\n",
      "accuracy = 0.856867\n",
      "loss = 48.131820\n",
      "accuracy = 0.856867\n",
      "loss = 48.123872\n",
      "accuracy = 0.856867\n",
      "loss = 48.087099\n",
      "accuracy = 0.856867\n",
      "loss = 48.078929\n",
      "accuracy = 0.856867\n",
      "loss = 48.042009\n",
      "accuracy = 0.856867\n",
      "loss = 48.025156\n",
      "accuracy = 0.856867\n",
      "loss = 47.995577\n",
      "accuracy = 0.856867\n",
      "loss = 47.981306\n",
      "accuracy = 0.856867\n",
      "loss = 47.951404\n",
      "accuracy = 0.856867\n",
      "loss = 47.936962\n",
      "accuracy = 0.856867\n",
      "loss = 47.906832\n",
      "accuracy = 0.856867\n",
      "loss = 47.883890\n",
      "accuracy = 0.856867\n",
      "loss = 47.869433\n",
      "accuracy = 0.856867\n",
      "loss = 47.837327\n",
      "accuracy = 0.856867\n",
      "loss = 47.821834\n",
      "accuracy = 0.856867\n",
      "loss = 47.791364\n",
      "accuracy = 0.856867\n",
      "loss = 47.784150\n",
      "accuracy = 0.856867\n",
      "loss = 47.751076\n",
      "accuracy = 0.856867\n",
      "loss = 47.737289\n",
      "accuracy = 0.856867\n",
      "loss = 47.705657\n",
      "accuracy = 0.856867\n",
      "loss = 47.698648\n",
      "accuracy = 0.856867\n",
      "loss = 47.665023\n",
      "accuracy = 0.856867\n",
      "loss = 47.639818\n",
      "accuracy = 0.856867\n",
      "loss = 47.626837\n",
      "accuracy = 0.854932\n",
      "loss = 47.600667\n",
      "accuracy = 0.854932\n",
      "loss = 47.577978\n",
      "accuracy = 0.854932\n",
      "loss = 47.559697\n",
      "accuracy = 0.854932\n",
      "loss = 47.536732\n",
      "accuracy = 0.854932\n",
      "loss = 47.510888\n",
      "accuracy = 0.854932\n",
      "loss = 47.497496\n",
      "accuracy = 0.854932\n",
      "loss = 47.464843\n",
      "accuracy = 0.854932\n",
      "loss = 47.453500\n",
      "accuracy = 0.854932\n",
      "loss = 47.418314\n",
      "accuracy = 0.854932\n",
      "loss = 47.403246\n",
      "accuracy = 0.854932\n",
      "loss = 47.386951\n",
      "accuracy = 0.854932\n",
      "loss = 47.352201\n",
      "accuracy = 0.854932\n",
      "loss = 47.336554\n",
      "accuracy = 0.854932\n",
      "loss = 47.319647\n",
      "accuracy = 0.854932\n",
      "loss = 47.288516\n",
      "accuracy = 0.854932\n",
      "loss = 47.272840\n",
      "accuracy = 0.854932\n",
      "loss = 47.253890\n",
      "accuracy = 0.854932\n",
      "loss = 47.238998\n",
      "accuracy = 0.854932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 47.200793\n",
      "accuracy = 0.854932\n",
      "loss = 47.189757\n",
      "accuracy = 0.854932\n",
      "loss = 47.172710\n",
      "accuracy = 0.854932\n",
      "loss = 47.134935\n",
      "accuracy = 0.854932\n",
      "loss = 47.123244\n",
      "accuracy = 0.854932\n",
      "loss = 47.102380\n",
      "accuracy = 0.854932\n",
      "loss = 47.086248\n",
      "accuracy = 0.854932\n",
      "loss = 47.048647\n",
      "accuracy = 0.854932\n",
      "loss = 47.035881\n",
      "accuracy = 0.854932\n",
      "loss = 47.019209\n",
      "accuracy = 0.856867\n",
      "loss = 46.985052\n",
      "accuracy = 0.854932\n",
      "loss = 46.976453\n",
      "accuracy = 0.856867\n",
      "loss = 159.597040\n",
      "accuracy = 0.450867\n",
      "loss = 156.825526\n",
      "accuracy = 0.450867\n",
      "loss = 154.051901\n",
      "accuracy = 0.450867\n",
      "loss = 151.276221\n",
      "accuracy = 0.450867\n",
      "loss = 148.498540\n",
      "accuracy = 0.456647\n",
      "loss = 145.718916\n",
      "accuracy = 0.450867\n",
      "loss = 142.937850\n",
      "accuracy = 0.450867\n",
      "loss = 140.198947\n",
      "accuracy = 0.445087\n",
      "loss = 137.458269\n",
      "accuracy = 0.445087\n",
      "loss = 134.728772\n",
      "accuracy = 0.445087\n",
      "loss = 132.057833\n",
      "accuracy = 0.445087\n",
      "loss = 129.399243\n",
      "accuracy = 0.445087\n",
      "loss = 126.771437\n",
      "accuracy = 0.445087\n",
      "loss = 124.178101\n",
      "accuracy = 0.456647\n",
      "loss = 121.665201\n",
      "accuracy = 0.456647\n",
      "loss = 119.221489\n",
      "accuracy = 0.456647\n",
      "loss = 116.892208\n",
      "accuracy = 0.462428\n",
      "loss = 114.636215\n",
      "accuracy = 0.479769\n",
      "loss = 112.412666\n",
      "accuracy = 0.485549\n",
      "loss = 110.208518\n",
      "accuracy = 0.479769\n",
      "loss = 108.032092\n",
      "accuracy = 0.479769\n",
      "loss = 105.862665\n",
      "accuracy = 0.479769\n",
      "loss = 103.749229\n",
      "accuracy = 0.479769\n",
      "loss = 101.634762\n",
      "accuracy = 0.479769\n",
      "loss = 99.552989\n",
      "accuracy = 0.473988\n",
      "loss = 97.593380\n",
      "accuracy = 0.479769\n",
      "loss = 95.738345\n",
      "accuracy = 0.485549\n",
      "loss = 93.987433\n",
      "accuracy = 0.473988\n",
      "loss = 92.240864\n",
      "accuracy = 0.468208\n",
      "loss = 90.532128\n",
      "accuracy = 0.473988\n",
      "loss = 88.828782\n",
      "accuracy = 0.468208\n",
      "loss = 87.225087\n",
      "accuracy = 0.473988\n",
      "loss = 85.620668\n",
      "accuracy = 0.473988\n",
      "loss = 84.018598\n",
      "accuracy = 0.479769\n",
      "loss = 82.432679\n",
      "accuracy = 0.479769\n",
      "loss = 80.883301\n",
      "accuracy = 0.485549\n",
      "loss = 79.393099\n",
      "accuracy = 0.485549\n",
      "loss = 77.949157\n",
      "accuracy = 0.479769\n",
      "loss = 76.557775\n",
      "accuracy = 0.497110\n",
      "loss = 75.213151\n",
      "accuracy = 0.491329\n",
      "loss = 73.904805\n",
      "accuracy = 0.491329\n",
      "loss = 72.648217\n",
      "accuracy = 0.491329\n",
      "loss = 71.455900\n",
      "accuracy = 0.491329\n",
      "loss = 70.286332\n",
      "accuracy = 0.497110\n",
      "loss = 69.174189\n",
      "accuracy = 0.497110\n",
      "loss = 68.108261\n",
      "accuracy = 0.502890\n",
      "loss = 67.051553\n",
      "accuracy = 0.502890\n",
      "loss = 66.010508\n",
      "accuracy = 0.502890\n",
      "loss = 65.022160\n",
      "accuracy = 0.514451\n",
      "loss = 64.033438\n",
      "accuracy = 0.508671\n",
      "loss = 63.050051\n",
      "accuracy = 0.508671\n",
      "loss = 62.079859\n",
      "accuracy = 0.514451\n",
      "loss = 61.136219\n",
      "accuracy = 0.537572\n",
      "loss = 60.281169\n",
      "accuracy = 0.543353\n",
      "loss = 59.425818\n",
      "accuracy = 0.543353\n",
      "loss = 58.570182\n",
      "accuracy = 0.554913\n",
      "loss = 57.721479\n",
      "accuracy = 0.554913\n",
      "loss = 56.895345\n",
      "accuracy = 0.566474\n",
      "loss = 56.072400\n",
      "accuracy = 0.589595\n",
      "loss = 55.321145\n",
      "accuracy = 0.595376\n",
      "loss = 54.575656\n",
      "accuracy = 0.618497\n",
      "loss = 53.848601\n",
      "accuracy = 0.624277\n",
      "loss = 53.124817\n",
      "accuracy = 0.624277\n",
      "loss = 52.426722\n",
      "accuracy = 0.630058\n",
      "loss = 51.766012\n",
      "accuracy = 0.635838\n",
      "loss = 51.105116\n",
      "accuracy = 0.635838\n",
      "loss = 50.444046\n",
      "accuracy = 0.635838\n",
      "loss = 49.796380\n",
      "accuracy = 0.641618\n",
      "loss = 49.224983\n",
      "accuracy = 0.635838\n",
      "loss = 48.686413\n",
      "accuracy = 0.635838\n",
      "loss = 48.172516\n",
      "accuracy = 0.635838\n",
      "loss = 47.702364\n",
      "accuracy = 0.635838\n",
      "loss = 47.269500\n",
      "accuracy = 0.635838\n",
      "loss = 46.856230\n",
      "accuracy = 0.647399\n",
      "loss = 46.446026\n",
      "accuracy = 0.647399\n",
      "loss = 46.053243\n",
      "accuracy = 0.647399\n",
      "loss = 45.681681\n",
      "accuracy = 0.658960\n",
      "loss = 45.316660\n",
      "accuracy = 0.664740\n",
      "loss = 44.993826\n",
      "accuracy = 0.664740\n",
      "loss = 44.673698\n",
      "accuracy = 0.664740\n",
      "loss = 44.373711\n",
      "accuracy = 0.664740\n",
      "loss = 44.078971\n",
      "accuracy = 0.670520\n",
      "loss = 43.810202\n",
      "accuracy = 0.682081\n",
      "loss = 43.572851\n",
      "accuracy = 0.676301\n",
      "loss = 43.339940\n",
      "accuracy = 0.676301\n",
      "loss = 43.119709\n",
      "accuracy = 0.676301\n",
      "loss = 42.899305\n",
      "accuracy = 0.676301\n",
      "loss = 42.678730\n",
      "accuracy = 0.670520\n",
      "loss = 42.457986\n",
      "accuracy = 0.676301\n",
      "loss = 42.237076\n",
      "accuracy = 0.682081\n",
      "loss = 42.016000\n",
      "accuracy = 0.682081\n",
      "loss = 41.794762\n",
      "accuracy = 0.682081\n",
      "loss = 41.573363\n",
      "accuracy = 0.682081\n",
      "loss = 41.351806\n",
      "accuracy = 0.687861\n",
      "loss = 41.130092\n",
      "accuracy = 0.687861\n",
      "loss = 40.911460\n",
      "accuracy = 0.687861\n",
      "loss = 40.707271\n",
      "accuracy = 0.687861\n",
      "loss = 40.510314\n",
      "accuracy = 0.687861\n",
      "loss = 40.313202\n",
      "accuracy = 0.687861\n",
      "loss = 40.115937\n",
      "accuracy = 0.693642\n",
      "loss = 39.918521\n",
      "accuracy = 0.699422\n",
      "loss = 39.720956\n",
      "accuracy = 0.699422\n",
      "loss = 39.523243\n",
      "accuracy = 0.699422\n",
      "loss = 39.325385\n",
      "accuracy = 0.699422\n",
      "loss = 39.127383\n",
      "accuracy = 0.699422\n",
      "loss = 38.929240\n",
      "accuracy = 0.699422\n",
      "loss = 38.734208\n",
      "accuracy = 0.699422\n",
      "loss = 38.554144\n",
      "accuracy = 0.699422\n",
      "loss = 38.374482\n",
      "accuracy = 0.699422\n",
      "loss = 38.194822\n",
      "accuracy = 0.699422\n",
      "loss = 38.015326\n",
      "accuracy = 0.699422\n",
      "loss = 37.832451\n",
      "accuracy = 0.699422\n",
      "loss = 37.650526\n",
      "accuracy = 0.699422\n",
      "loss = 37.481752\n",
      "accuracy = 0.699422\n",
      "loss = 37.312851\n",
      "accuracy = 0.699422\n",
      "loss = 37.143824\n",
      "accuracy = 0.699422\n",
      "loss = 36.974674\n",
      "accuracy = 0.699422\n",
      "loss = 36.805402\n",
      "accuracy = 0.699422\n",
      "loss = 36.636009\n",
      "accuracy = 0.699422\n",
      "loss = 36.466496\n",
      "accuracy = 0.699422\n",
      "loss = 36.296866\n",
      "accuracy = 0.699422\n",
      "loss = 36.127119\n",
      "accuracy = 0.699422\n",
      "loss = 35.962805\n",
      "accuracy = 0.699422\n",
      "loss = 35.816237\n",
      "accuracy = 0.699422\n",
      "loss = 35.669552\n",
      "accuracy = 0.699422\n",
      "loss = 35.522750\n",
      "accuracy = 0.705202\n",
      "loss = 35.375834\n",
      "accuracy = 0.710983\n",
      "loss = 35.228804\n",
      "accuracy = 0.710983\n",
      "loss = 35.081661\n",
      "accuracy = 0.710983\n",
      "loss = 34.935795\n",
      "accuracy = 0.710983\n",
      "loss = 34.791044\n",
      "accuracy = 0.716763\n",
      "loss = 34.646181\n",
      "accuracy = 0.716763\n",
      "loss = 34.503301\n",
      "accuracy = 0.722543\n",
      "loss = 34.364661\n",
      "accuracy = 0.722543\n",
      "loss = 34.249558\n",
      "accuracy = 0.722543\n",
      "loss = 34.135850\n",
      "accuracy = 0.722543\n",
      "loss = 34.031215\n",
      "accuracy = 0.722543\n",
      "loss = 33.926871\n",
      "accuracy = 0.722543\n",
      "loss = 33.831431\n",
      "accuracy = 0.722543\n",
      "loss = 33.737432\n",
      "accuracy = 0.722543\n",
      "loss = 33.652502\n",
      "accuracy = 0.722543\n",
      "loss = 33.567486\n",
      "accuracy = 0.722543\n",
      "loss = 33.491187\n",
      "accuracy = 0.716763\n",
      "loss = 33.414775\n",
      "accuracy = 0.716763\n",
      "loss = 33.338250\n",
      "accuracy = 0.722543\n",
      "loss = 33.261614\n",
      "accuracy = 0.728324\n",
      "loss = 33.186710\n",
      "accuracy = 0.734104\n",
      "loss = 33.112570\n",
      "accuracy = 0.734104\n",
      "loss = 33.038317\n",
      "accuracy = 0.734104\n",
      "loss = 32.963950\n",
      "accuracy = 0.734104\n",
      "loss = 32.889472\n",
      "accuracy = 0.734104\n",
      "loss = 32.814882\n",
      "accuracy = 0.734104\n",
      "loss = 32.740181\n",
      "accuracy = 0.734104\n",
      "loss = 32.665564\n",
      "accuracy = 0.734104\n",
      "loss = 32.600234\n",
      "accuracy = 0.739884\n",
      "loss = 32.534794\n",
      "accuracy = 0.739884\n",
      "loss = 32.469246\n",
      "accuracy = 0.739884\n",
      "loss = 32.403589\n",
      "accuracy = 0.739884\n",
      "loss = 32.337825\n",
      "accuracy = 0.745665\n",
      "loss = 32.271954\n",
      "accuracy = 0.745665\n",
      "loss = 32.205977\n",
      "accuracy = 0.745665\n",
      "loss = 32.139894\n",
      "accuracy = 0.751445\n",
      "loss = 32.073707\n",
      "accuracy = 0.751445\n",
      "loss = 32.007882\n",
      "accuracy = 0.751445\n",
      "loss = 31.944888\n",
      "accuracy = 0.751445\n",
      "loss = 31.881788\n",
      "accuracy = 0.751445\n",
      "loss = 31.818581\n",
      "accuracy = 0.751445\n",
      "loss = 31.755270\n",
      "accuracy = 0.751445\n",
      "loss = 31.691855\n",
      "accuracy = 0.751445\n",
      "loss = 31.628556\n",
      "accuracy = 0.751445\n",
      "loss = 31.559756\n",
      "accuracy = 0.751445\n",
      "loss = 31.496771\n",
      "accuracy = 0.745665\n",
      "loss = 31.429471\n",
      "accuracy = 0.745665\n",
      "loss = 31.367502\n",
      "accuracy = 0.745665\n",
      "loss = 31.305429\n",
      "accuracy = 0.745665\n",
      "loss = 31.243251\n",
      "accuracy = 0.745665\n",
      "loss = 31.180969\n",
      "accuracy = 0.745665\n",
      "loss = 31.119244\n",
      "accuracy = 0.745665\n",
      "loss = 31.056399\n",
      "accuracy = 0.745665\n",
      "loss = 30.993937\n",
      "accuracy = 0.745665\n",
      "loss = 30.926463\n",
      "accuracy = 0.745665\n",
      "loss = 30.858877\n",
      "accuracy = 0.745665\n",
      "loss = 30.791180\n",
      "accuracy = 0.745665\n",
      "loss = 30.723372\n",
      "accuracy = 0.745665\n",
      "loss = 30.655453\n",
      "accuracy = 0.745665\n",
      "loss = 30.587425\n",
      "accuracy = 0.745665\n",
      "loss = 30.519286\n",
      "accuracy = 0.745665\n",
      "loss = 30.451039\n",
      "accuracy = 0.745665\n",
      "loss = 30.383213\n",
      "accuracy = 0.745665\n",
      "loss = 30.315377\n",
      "accuracy = 0.745665\n",
      "loss = 30.247437\n",
      "accuracy = 0.745665\n",
      "loss = 30.179555\n",
      "accuracy = 0.745665\n",
      "loss = 30.113382\n",
      "accuracy = 0.745665\n",
      "loss = 30.045460\n",
      "accuracy = 0.745665\n",
      "loss = 29.979024\n",
      "accuracy = 0.745665\n",
      "loss = 29.912612\n",
      "accuracy = 0.745665\n",
      "loss = 29.844335\n",
      "accuracy = 0.745665\n",
      "loss = 29.777605\n",
      "accuracy = 0.745665\n",
      "loss = 29.710951\n",
      "accuracy = 0.745665\n",
      "loss = 29.644458\n",
      "accuracy = 0.745665\n",
      "loss = 29.588409\n",
      "accuracy = 0.745665\n",
      "loss = 29.530623\n",
      "accuracy = 0.745665\n",
      "loss = 29.474436\n",
      "accuracy = 0.745665\n",
      "loss = 29.416401\n",
      "accuracy = 0.745665\n",
      "loss = 29.360075\n",
      "accuracy = 0.745665\n",
      "loss = 29.301796\n",
      "accuracy = 0.745665\n",
      "loss = 29.245328\n",
      "accuracy = 0.745665\n",
      "loss = 29.186809\n",
      "accuracy = 0.745665\n",
      "loss = 29.130198\n",
      "accuracy = 0.745665\n",
      "loss = 29.071483\n",
      "accuracy = 0.745665\n",
      "loss = 29.012918\n",
      "accuracy = 0.745665\n",
      "loss = 28.955783\n",
      "accuracy = 0.745665\n",
      "loss = 28.896990\n",
      "accuracy = 0.745665\n",
      "loss = 28.839708\n",
      "accuracy = 0.745665\n",
      "loss = 28.781691\n",
      "accuracy = 0.745665\n",
      "loss = 28.734744\n",
      "accuracy = 0.751445\n",
      "loss = 28.686181\n",
      "accuracy = 0.751445\n",
      "loss = 28.637525\n",
      "accuracy = 0.751445\n",
      "loss = 28.588777\n",
      "accuracy = 0.751445\n",
      "loss = 28.539936\n",
      "accuracy = 0.751445\n",
      "loss = 28.491003\n",
      "accuracy = 0.751445\n",
      "loss = 28.441978\n",
      "accuracy = 0.751445\n",
      "loss = 28.392862\n",
      "accuracy = 0.757225\n",
      "loss = 28.343655\n",
      "accuracy = 0.757225\n",
      "loss = 28.294358\n",
      "accuracy = 0.757225\n",
      "loss = 28.244970\n",
      "accuracy = 0.757225\n",
      "loss = 28.195492\n",
      "accuracy = 0.757225\n",
      "loss = 28.145925\n",
      "accuracy = 0.757225\n",
      "loss = 28.096269\n",
      "accuracy = 0.757225\n",
      "loss = 28.046524\n",
      "accuracy = 0.757225\n",
      "loss = 27.996691\n",
      "accuracy = 0.757225\n",
      "loss = 27.946770\n",
      "accuracy = 0.757225\n",
      "loss = 27.896761\n",
      "accuracy = 0.757225\n",
      "loss = 27.846665\n",
      "accuracy = 0.751445\n",
      "loss = 27.796482\n",
      "accuracy = 0.751445\n",
      "loss = 27.746899\n",
      "accuracy = 0.757225\n",
      "loss = 27.695827\n",
      "accuracy = 0.757225\n",
      "loss = 27.645343\n",
      "accuracy = 0.757225\n",
      "loss = 27.602764\n",
      "accuracy = 0.757225\n",
      "loss = 27.560096\n",
      "accuracy = 0.757225\n",
      "loss = 27.517411\n",
      "accuracy = 0.763006\n",
      "loss = 27.476290\n",
      "accuracy = 0.763006\n",
      "loss = 27.433357\n",
      "accuracy = 0.768786\n",
      "loss = 27.390335\n",
      "accuracy = 0.768786\n",
      "loss = 27.347226\n",
      "accuracy = 0.768786\n",
      "loss = 27.304028\n",
      "accuracy = 0.774566\n",
      "loss = 27.260747\n",
      "accuracy = 0.774566\n",
      "loss = 27.219168\n",
      "accuracy = 0.774566\n",
      "loss = 27.175711\n",
      "accuracy = 0.774566\n",
      "loss = 27.132168\n",
      "accuracy = 0.774566\n",
      "loss = 27.088538\n",
      "accuracy = 0.774566\n",
      "loss = 27.044823\n",
      "accuracy = 0.774566\n",
      "loss = 27.001021\n",
      "accuracy = 0.774566\n",
      "loss = 26.957196\n",
      "accuracy = 0.774566\n",
      "loss = 26.914960\n",
      "accuracy = 0.774566\n",
      "loss = 26.870907\n",
      "accuracy = 0.774566\n",
      "loss = 26.826769\n",
      "accuracy = 0.774566\n",
      "loss = 26.782547\n",
      "accuracy = 0.774566\n",
      "loss = 26.738241\n",
      "accuracy = 0.774566\n",
      "loss = 26.693907\n",
      "accuracy = 0.774566\n",
      "loss = 26.651178\n",
      "accuracy = 0.774566\n",
      "loss = 26.606627\n",
      "accuracy = 0.774566\n",
      "loss = 26.561993\n",
      "accuracy = 0.774566\n",
      "loss = 26.517276\n",
      "accuracy = 0.774566\n",
      "loss = 26.472478\n",
      "accuracy = 0.768786\n",
      "loss = 26.427673\n",
      "accuracy = 0.774566\n",
      "loss = 26.384436\n",
      "accuracy = 0.780347\n",
      "loss = 26.339399\n",
      "accuracy = 0.780347\n",
      "loss = 26.294281\n",
      "accuracy = 0.780347\n",
      "loss = 26.249082\n",
      "accuracy = 0.780347\n",
      "loss = 26.203813\n",
      "accuracy = 0.780347\n",
      "loss = 26.160245\n",
      "accuracy = 0.780347\n",
      "loss = 26.114812\n",
      "accuracy = 0.780347\n",
      "loss = 26.069301\n",
      "accuracy = 0.780347\n",
      "loss = 26.023710\n",
      "accuracy = 0.780347\n",
      "loss = 25.978042\n",
      "accuracy = 0.780347\n",
      "loss = 25.932372\n",
      "accuracy = 0.780347\n",
      "loss = 25.888271\n",
      "accuracy = 0.780347\n",
      "loss = 25.842374\n",
      "accuracy = 0.780347\n",
      "loss = 25.796401\n",
      "accuracy = 0.780347\n",
      "loss = 25.750351\n",
      "accuracy = 0.780347\n",
      "loss = 25.704274\n",
      "accuracy = 0.780347\n",
      "loss = 25.659822\n",
      "accuracy = 0.780347\n",
      "loss = 25.613549\n",
      "accuracy = 0.780347\n",
      "loss = 25.567201\n",
      "accuracy = 0.780347\n",
      "loss = 25.520778\n",
      "accuracy = 0.780347\n",
      "loss = 25.474320\n",
      "accuracy = 0.780347\n",
      "loss = 25.429509\n",
      "accuracy = 0.780347\n",
      "loss = 25.382868\n",
      "accuracy = 0.780347\n",
      "loss = 25.336380\n",
      "accuracy = 0.774566\n",
      "loss = 25.297225\n",
      "accuracy = 0.774566\n",
      "loss = 25.257996\n",
      "accuracy = 0.774566\n",
      "loss = 25.218695\n",
      "accuracy = 0.774566\n",
      "loss = 25.179341\n",
      "accuracy = 0.774566\n",
      "loss = 25.141607\n",
      "accuracy = 0.780347\n",
      "loss = 25.102095\n",
      "accuracy = 0.780347\n",
      "loss = 25.062512\n",
      "accuracy = 0.780347\n",
      "loss = 25.022858\n",
      "accuracy = 0.780347\n",
      "loss = 24.983211\n",
      "accuracy = 0.774566\n",
      "loss = 24.950896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.780347\n",
      "loss = 24.918512\n",
      "accuracy = 0.780347\n",
      "loss = 24.886780\n",
      "accuracy = 0.780347\n",
      "loss = 24.853743\n",
      "accuracy = 0.774566\n",
      "loss = 24.822484\n",
      "accuracy = 0.780347\n",
      "loss = 24.789356\n",
      "accuracy = 0.774566\n",
      "loss = 24.758092\n",
      "accuracy = 0.774566\n",
      "loss = 24.725758\n",
      "accuracy = 0.774566\n",
      "loss = 24.694411\n",
      "accuracy = 0.780347\n",
      "loss = 24.664139\n",
      "accuracy = 0.780347\n",
      "loss = 24.639902\n",
      "accuracy = 0.780347\n",
      "loss = 24.616975\n",
      "accuracy = 0.780347\n",
      "loss = 24.601219\n",
      "accuracy = 0.780347\n",
      "loss = 24.579558\n",
      "accuracy = 0.780347\n",
      "loss = 24.565123\n",
      "accuracy = 0.780347\n",
      "loss = 24.549153\n",
      "accuracy = 0.780347\n",
      "loss = 24.533195\n",
      "accuracy = 0.780347\n",
      "loss = 24.511399\n",
      "accuracy = 0.780347\n",
      "loss = 24.496511\n",
      "accuracy = 0.780347\n",
      "loss = 24.480379\n",
      "accuracy = 0.780347\n",
      "loss = 24.459650\n",
      "accuracy = 0.780347\n",
      "loss = 24.443365\n",
      "accuracy = 0.780347\n",
      "loss = 24.428968\n",
      "accuracy = 0.780347\n",
      "loss = 24.406122\n",
      "accuracy = 0.780347\n",
      "loss = 24.391435\n",
      "accuracy = 0.780347\n",
      "loss = 24.368633\n",
      "accuracy = 0.780347\n",
      "loss = 24.353650\n",
      "accuracy = 0.780347\n",
      "loss = 24.336975\n",
      "accuracy = 0.780347\n",
      "loss = 24.314212\n",
      "accuracy = 0.780347\n",
      "loss = 24.298928\n",
      "accuracy = 0.780347\n",
      "loss = 24.282009\n",
      "accuracy = 0.780347\n",
      "loss = 24.265026\n",
      "accuracy = 0.780347\n",
      "loss = 24.248022\n",
      "accuracy = 0.780347\n",
      "loss = 24.226537\n",
      "accuracy = 0.780347\n",
      "loss = 24.209401\n",
      "accuracy = 0.780347\n",
      "loss = 24.194146\n",
      "accuracy = 0.780347\n",
      "loss = 24.170566\n",
      "accuracy = 0.780347\n",
      "loss = 24.154949\n",
      "accuracy = 0.780347\n",
      "loss = 24.131496\n",
      "accuracy = 0.780347\n",
      "loss = 24.115726\n",
      "accuracy = 0.780347\n",
      "loss = 24.098202\n",
      "accuracy = 0.780347\n",
      "loss = 24.080617\n",
      "accuracy = 0.780347\n",
      "loss = 24.063236\n",
      "accuracy = 0.780347\n",
      "loss = 24.045580\n",
      "accuracy = 0.780347\n",
      "loss = 24.029671\n",
      "accuracy = 0.780347\n",
      "loss = 24.013309\n",
      "accuracy = 0.780347\n",
      "loss = 23.991228\n",
      "accuracy = 0.780347\n",
      "loss = 23.974507\n",
      "accuracy = 0.786127\n",
      "loss = 23.952558\n",
      "accuracy = 0.786127\n",
      "loss = 23.935563\n",
      "accuracy = 0.786127\n",
      "loss = 23.919421\n",
      "accuracy = 0.786127\n",
      "loss = 23.895296\n",
      "accuracy = 0.786127\n",
      "loss = 23.878130\n",
      "accuracy = 0.786127\n",
      "loss = 23.861281\n",
      "accuracy = 0.786127\n",
      "loss = 23.837524\n",
      "accuracy = 0.786127\n",
      "loss = 23.820188\n",
      "accuracy = 0.786127\n",
      "loss = 23.802836\n",
      "accuracy = 0.786127\n",
      "loss = 23.786312\n",
      "accuracy = 0.786127\n",
      "loss = 23.763009\n",
      "accuracy = 0.786127\n",
      "loss = 23.746043\n",
      "accuracy = 0.786127\n",
      "loss = 23.721744\n",
      "accuracy = 0.786127\n",
      "loss = 23.704081\n",
      "accuracy = 0.786127\n",
      "loss = 23.686492\n",
      "accuracy = 0.786127\n",
      "loss = 23.669511\n",
      "accuracy = 0.786127\n",
      "loss = 23.645956\n",
      "accuracy = 0.786127\n",
      "loss = 23.628908\n",
      "accuracy = 0.786127\n",
      "loss = 23.611649\n",
      "accuracy = 0.791908\n",
      "loss = 23.586889\n",
      "accuracy = 0.791908\n",
      "loss = 23.568852\n",
      "accuracy = 0.791908\n",
      "loss = 23.550838\n",
      "accuracy = 0.791908\n",
      "loss = 23.532218\n",
      "accuracy = 0.791908\n",
      "loss = 23.514028\n",
      "accuracy = 0.791908\n",
      "loss = 23.490073\n",
      "accuracy = 0.791908\n",
      "loss = 23.472149\n",
      "accuracy = 0.791908\n",
      "loss = 23.447537\n",
      "accuracy = 0.791908\n",
      "loss = 23.430097\n",
      "accuracy = 0.791908\n",
      "loss = 23.413001\n",
      "accuracy = 0.791908\n",
      "loss = 23.396785\n",
      "accuracy = 0.791908\n",
      "loss = 23.378952\n",
      "accuracy = 0.791908\n",
      "loss = 23.362682\n",
      "accuracy = 0.791908\n",
      "loss = 23.344702\n",
      "accuracy = 0.791908\n",
      "loss = 23.328379\n",
      "accuracy = 0.791908\n",
      "loss = 23.310255\n",
      "accuracy = 0.791908\n",
      "loss = 23.293876\n",
      "accuracy = 0.791908\n",
      "loss = 23.275612\n",
      "accuracy = 0.791908\n",
      "loss = 23.259177\n",
      "accuracy = 0.797688\n",
      "loss = 23.240804\n",
      "accuracy = 0.797688\n",
      "loss = 23.222636\n",
      "accuracy = 0.797688\n",
      "loss = 23.205807\n",
      "accuracy = 0.797688\n",
      "loss = 23.187506\n",
      "accuracy = 0.797688\n",
      "loss = 23.170617\n",
      "accuracy = 0.797688\n",
      "loss = 23.152186\n",
      "accuracy = 0.797688\n",
      "loss = 23.135236\n",
      "accuracy = 0.797688\n",
      "loss = 23.116678\n",
      "accuracy = 0.803468\n",
      "loss = 23.099861\n",
      "accuracy = 0.803468\n",
      "loss = 23.081589\n",
      "accuracy = 0.803468\n",
      "loss = 23.064829\n",
      "accuracy = 0.803468\n",
      "loss = 23.046382\n",
      "accuracy = 0.803468\n",
      "loss = 23.029451\n",
      "accuracy = 0.803468\n",
      "loss = 23.012874\n",
      "accuracy = 0.803468\n",
      "loss = 22.995729\n",
      "accuracy = 0.803468\n",
      "loss = 22.979184\n",
      "accuracy = 0.803468\n",
      "loss = 22.961917\n",
      "accuracy = 0.803468\n",
      "loss = 22.944874\n",
      "accuracy = 0.803468\n",
      "loss = 22.926598\n",
      "accuracy = 0.803468\n",
      "loss = 22.909194\n",
      "accuracy = 0.803468\n",
      "loss = 22.891748\n",
      "accuracy = 0.803468\n",
      "loss = 22.874443\n",
      "accuracy = 0.803468\n",
      "loss = 22.855369\n",
      "accuracy = 0.803468\n",
      "loss = 22.837857\n",
      "accuracy = 0.803468\n",
      "loss = 22.819523\n",
      "accuracy = 0.803468\n",
      "loss = 22.801857\n",
      "accuracy = 0.803468\n",
      "loss = 22.784152\n",
      "accuracy = 0.803468\n",
      "loss = 22.766457\n",
      "accuracy = 0.803468\n",
      "loss = 22.747256\n",
      "accuracy = 0.803468\n",
      "loss = 22.729426\n",
      "accuracy = 0.803468\n",
      "loss = 22.711826\n",
      "accuracy = 0.803468\n",
      "loss = 22.692991\n",
      "accuracy = 0.803468\n",
      "loss = 22.675033\n",
      "accuracy = 0.803468\n",
      "loss = 22.657037\n",
      "accuracy = 0.803468\n",
      "loss = 22.639160\n",
      "accuracy = 0.803468\n",
      "loss = 22.620272\n",
      "accuracy = 0.803468\n",
      "loss = 22.602150\n",
      "accuracy = 0.803468\n",
      "loss = 22.583991\n",
      "accuracy = 0.803468\n",
      "loss = 22.565913\n",
      "accuracy = 0.803468\n",
      "loss = 22.546195\n",
      "accuracy = 0.803468\n",
      "loss = 22.527919\n",
      "accuracy = 0.803468\n",
      "loss = 22.509949\n",
      "accuracy = 0.803468\n",
      "loss = 22.490599\n",
      "accuracy = 0.803468\n",
      "loss = 22.472203\n",
      "accuracy = 0.803468\n",
      "loss = 22.453770\n",
      "accuracy = 0.803468\n",
      "loss = 22.435520\n",
      "accuracy = 0.803468\n",
      "loss = 22.416140\n",
      "accuracy = 0.803468\n",
      "loss = 22.397589\n",
      "accuracy = 0.803468\n",
      "loss = 22.379003\n",
      "accuracy = 0.803468\n",
      "loss = 22.360519\n",
      "accuracy = 0.803468\n",
      "loss = 22.340361\n",
      "accuracy = 0.803468\n",
      "loss = 22.321666\n",
      "accuracy = 0.803468\n",
      "loss = 22.303320\n",
      "accuracy = 0.803468\n",
      "loss = 22.283515\n",
      "accuracy = 0.803468\n",
      "loss = 22.264707\n",
      "accuracy = 0.803468\n",
      "loss = 22.245865\n",
      "accuracy = 0.803468\n",
      "loss = 22.227237\n",
      "accuracy = 0.803468\n",
      "loss = 22.207423\n",
      "accuracy = 0.803468\n",
      "loss = 22.188471\n",
      "accuracy = 0.803468\n",
      "loss = 22.169485\n",
      "accuracy = 0.803468\n",
      "loss = 22.150575\n",
      "accuracy = 0.809249\n",
      "loss = 22.130053\n",
      "accuracy = 0.809249\n",
      "loss = 22.110993\n",
      "accuracy = 0.809249\n",
      "loss = 22.092554\n",
      "accuracy = 0.809249\n",
      "loss = 22.073444\n",
      "accuracy = 0.809249\n",
      "loss = 22.052845\n",
      "accuracy = 0.809249\n",
      "loss = 22.033621\n",
      "accuracy = 0.809249\n",
      "loss = 22.014612\n",
      "accuracy = 0.809249\n",
      "loss = 21.994423\n",
      "accuracy = 0.809249\n",
      "loss = 21.975096\n",
      "accuracy = 0.809249\n",
      "loss = 21.955739\n",
      "accuracy = 0.809249\n",
      "loss = 21.936448\n",
      "accuracy = 0.809249\n",
      "loss = 21.916278\n",
      "accuracy = 0.809249\n",
      "loss = 21.896820\n",
      "accuracy = 0.815029\n",
      "loss = 21.877333\n",
      "accuracy = 0.815029\n",
      "loss = 21.857818\n",
      "accuracy = 0.815029\n",
      "loss = 21.838628\n",
      "accuracy = 0.815029\n",
      "loss = 21.818044\n",
      "accuracy = 0.815029\n",
      "loss = 21.798431\n",
      "accuracy = 0.815029\n",
      "loss = 21.778791\n",
      "accuracy = 0.815029\n",
      "loss = 21.759319\n",
      "accuracy = 0.815029\n",
      "loss = 21.738770\n",
      "accuracy = 0.809249\n",
      "loss = 21.719035\n",
      "accuracy = 0.809249\n",
      "loss = 21.699273\n",
      "accuracy = 0.809249\n",
      "loss = 21.679519\n",
      "accuracy = 0.809249\n",
      "loss = 21.659012\n",
      "accuracy = 0.809249\n",
      "loss = 21.639159\n",
      "accuracy = 0.809249\n",
      "loss = 21.619280\n",
      "accuracy = 0.815029\n",
      "loss = 21.599375\n",
      "accuracy = 0.815029\n",
      "loss = 21.579724\n",
      "accuracy = 0.815029\n",
      "loss = 21.558832\n",
      "accuracy = 0.815029\n",
      "loss = 21.538839\n",
      "accuracy = 0.815029\n",
      "loss = 21.518821\n",
      "accuracy = 0.815029\n",
      "loss = 21.498889\n",
      "accuracy = 0.815029\n",
      "loss = 21.478055\n",
      "accuracy = 0.815029\n",
      "loss = 21.457952\n",
      "accuracy = 0.815029\n",
      "loss = 21.437825\n",
      "accuracy = 0.815029\n",
      "loss = 21.418240\n",
      "accuracy = 0.815029\n",
      "loss = 21.398769\n",
      "accuracy = 0.815029\n",
      "loss = 21.379077\n",
      "accuracy = 0.815029\n",
      "loss = 21.359357\n",
      "accuracy = 0.815029\n",
      "loss = 21.339607\n",
      "accuracy = 0.815029\n",
      "loss = 21.319829\n",
      "accuracy = 0.815029\n",
      "loss = 21.300022\n",
      "accuracy = 0.815029\n",
      "loss = 21.280187\n",
      "accuracy = 0.815029\n",
      "loss = 21.261042\n",
      "accuracy = 0.815029\n",
      "loss = 21.249966\n",
      "accuracy = 0.820809\n",
      "loss = 21.238863\n",
      "accuracy = 0.820809\n",
      "loss = 21.227734\n",
      "accuracy = 0.826590\n",
      "loss = 21.216579\n",
      "accuracy = 0.826590\n",
      "loss = 21.205398\n",
      "accuracy = 0.826590\n",
      "loss = 21.194192\n",
      "accuracy = 0.826590\n",
      "loss = 21.182960\n",
      "accuracy = 0.826590\n",
      "loss = 21.171703\n",
      "accuracy = 0.826590\n",
      "loss = 21.160421\n",
      "accuracy = 0.826590\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmJD+H6AjBEVLMeF+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1fd123a37f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def loadDataSet():\n",
    "\t# 读取数据\n",
    "\tX,y=load_svmlight_file(\"australian_scale.txt\")\n",
    "\t#将稀疏矩阵转化为完整特征矩阵\n",
    "\tX = X.todense()\n",
    "\t# 将数据集切分为训练集和验证集 \n",
    "\tX_train, X_validation, y_train, y_validation = train_test_split(X, y, random_state=0)\n",
    "\tprint(X_train.shape,y_train.shape)\n",
    "\treturn X_train, X_validation, y_train, y_validation\n",
    "\n",
    "def gradDescent(alpha,maxCycles,X_data,y_data):\n",
    "\tnum = y_data.shape[0]    #样本数量\n",
    "\t# 线性模型参数正态分布初始化\n",
    "\tw = np.random.normal(size=(X_data.shape[1]))\n",
    "\tb = np.random.normal(size=1)\n",
    "\tlosss = []\n",
    "\ttv = 0\n",
    "\n",
    "\t#迭代次maxCycles次\n",
    "\tfor n in range(maxCycles):\n",
    "\t\tgrad_w = np.ones(X_data.shape[1])*(np.linalg.norm(w,ord=2))\n",
    "\t\tgrad_b = np.zeros(1)\n",
    "\t\tloss = 0\n",
    "\t\terror = 0\n",
    "\t\tC = 1/np.power(2,2)\n",
    "\t\tfor i in range(num):\n",
    "\t\t\ty = np.dot( X_data[i][0].getA()[0], w ) + b\n",
    "\t\t\tif y_data[i] * y < 1:\n",
    "\t\t\t\tloss += C * max(0,1 - y_data[i] * y) \n",
    "\t\t\t\tgrad_w += - C * y_data[i] * X_data[i][0].getA()[0] \n",
    "\t\t\t\tgrad_b += - C * y_data[i] \n",
    "\t\t\tif y > tv :\n",
    "\t\t\t\ty = 1\n",
    "\t\t\telse: y = -1\n",
    "\t\t\tif not y == y_data[i]:\n",
    "\t\t\t\terror += 1\n",
    "\t\t#更新模型参数\n",
    "\t\tw -= alpha * grad_w\n",
    "\t\tb -= alpha * grad_b\n",
    "\t\tlosss.append(loss)\n",
    "\t\tprint(\"loss = %f\" % loss)\n",
    "\t\tprint(\"accuracy = %f\" % (1-error/num))\n",
    "\treturn losss\n",
    "\n",
    "def plotLossPerTime(n,losss_train,losss_validation):\n",
    "\tplt.xlabel('iteration times')\n",
    "\tplt.ylabel('loss of train or validation')\n",
    "\tplt.title('linear classification & gradient decrease')\n",
    "\tn_cycles = range(1,n+1)\n",
    "\tplt.plot(n_cycles, losss_train, label = \"Loss of Train\", color='blue', linewidth=3)\n",
    "\tplt.plot(n_cycles, losss_validation, label = \"Loss of Validation\", color='red', linewidth=3)\n",
    "\tplt.legend(loc=0)\n",
    "\tplt.grid()\n",
    "\tplt.show()\n",
    "\n",
    "# main\n",
    "X_train, X_validation, y_train, y_validation = loadDataSet()\n",
    "alpha = 0.001\n",
    "maxCycles = 500\n",
    "losss_train = gradDescent(alpha,maxCycles,X_train,y_train)\n",
    "losss_validation = gradDescent(alpha,maxCycles,X_validation,y_validation)\n",
    "plotLossPerTime(maxCycles,losss_train,losss_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
