{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'housing_scale.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-91532768cf71>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;31m# main\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_validation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloadDataSet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[0mmaxCycles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-91532768cf71>\u001b[0m in \u001b[0;36mloadDataSet\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mloadDataSet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;31m# 读取数据\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mload_svmlight_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"housing_scale.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[1;31m#将稀疏矩阵转化为完整特征矩阵\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\datasets\\svmlight_format.py\u001b[0m in \u001b[0;36mload_svmlight_file\u001b[1;34m(f, n_features, dtype, multilabel, zero_based, query_id, offset, length)\u001b[0m\n\u001b[0;32m    145\u001b[0m     \"\"\"\n\u001b[0;32m    146\u001b[0m     return tuple(load_svmlight_files([f], n_features, dtype, multilabel,\n\u001b[1;32m--> 147\u001b[1;33m                                      zero_based, query_id, offset, length))\n\u001b[0m\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\datasets\\svmlight_format.py\u001b[0m in \u001b[0;36mload_svmlight_files\u001b[1;34m(files, n_features, dtype, multilabel, zero_based, query_id, offset, length)\u001b[0m\n\u001b[0;32m    288\u001b[0m     r = [_open_and_load(f, dtype, multilabel, bool(zero_based), bool(query_id),\n\u001b[0;32m    289\u001b[0m                         offset=offset, length=length)\n\u001b[1;32m--> 290\u001b[1;33m          for f in files]\n\u001b[0m\u001b[0;32m    291\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m     if (zero_based is False or\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\datasets\\svmlight_format.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    288\u001b[0m     r = [_open_and_load(f, dtype, multilabel, bool(zero_based), bool(query_id),\n\u001b[0;32m    289\u001b[0m                         offset=offset, length=length)\n\u001b[1;32m--> 290\u001b[1;33m          for f in files]\n\u001b[0m\u001b[0;32m    291\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m     if (zero_based is False or\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\datasets\\svmlight_format.py\u001b[0m in \u001b[0;36m_open_and_load\u001b[1;34m(f, dtype, multilabel, zero_based, query_id, offset, length)\u001b[0m\n\u001b[0;32m    173\u001b[0m     \u001b[1;31m# XXX remove closing when Python 2.7+/3.1+ required\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mclosing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_gen_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m             \u001b[0mactual_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindptr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m                 _load_svmlight_file(f, dtype, multilabel, zero_based, query_id,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\datasets\\svmlight_format.py\u001b[0m in \u001b[0;36m_gen_open\u001b[1;34m(f)\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mBZ2File\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 164\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'housing_scale.txt'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def loadDataSet():\n",
    "\t# 读取数据\n",
    "\tX,y=load_svmlight_file(\"housing_scale.txt\")\n",
    "\t#将稀疏矩阵转化为完整特征矩阵\n",
    "\tX.todense()\n",
    "\t# 将数据集切分为训练集和验证集\n",
    "\tX_train, X_validation, y_train, y_validation = train_test_split(X, y, random_state=0)\n",
    "\tprint(X_train.shape,y_train.shape)\n",
    "\treturn X_train, X_validation, y_train, y_validation\n",
    "\n",
    "def gradDescent(alpha,maxCycles,X_data,y_data):\n",
    "\tnum = y_data.shape[0]    #样本数量\n",
    "\t# 线性模型参数正态分布初始化\n",
    "\tw = np.random.normal(size=(X_data.shape[1]))\n",
    "\tb = np.random.normal(size=1)\n",
    "\tlosss = []\n",
    "\n",
    "\t#迭代次maxCycles次\n",
    "\tfor n in range(maxCycles):\n",
    "\t\tgrad_w = np.zeros(X_data.shape[1])\n",
    "\t\tgrad_b = np.zeros(1)\n",
    "\t\tloss = 0\n",
    "\t\tfor i in range(num):\n",
    "\t\t\ty = np.dot( X_data[i].data, w ) + b\n",
    "\t\t\tloss += np.power((y - y_data[i]),2) / ( 2 * num)\n",
    "\t\t\tgrad_w += ( y - y_data[i] ) * X_data[i].data / num\n",
    "\t\t\tgrad_b += ( y - y_data[i] ) / num\n",
    "\t\t#更新模型参数\n",
    "\t\tw -= alpha * grad_w \n",
    "\t\tb -= alpha * grad_b\n",
    "\t\tlosss.append(loss)\n",
    "\t\tprint(\"loss = %f\" % loss)\n",
    "\tprint(w)\n",
    "\tprint(b)\n",
    "\treturn losss\n",
    "\n",
    "def plotLossPerTime(n,losss_train,losss_validation):\n",
    "\tplt.xlabel('iteration times')\n",
    "\tplt.ylabel('loss of train or validation')\n",
    "\tplt.title('linear regression & gradient decrease')\n",
    "\tn_cycles = range(1,n+1)\n",
    "\tplt.plot(n_cycles, losss_train, label = \"Loss of Train\", color='blue', linewidth=3)\n",
    "\tplt.plot(n_cycles, losss_validation, label = \"Loss of Validation\", color='red', linewidth=3)\n",
    "\tplt.legend(loc=0)\n",
    "\tplt.grid()\n",
    "\tplt.show()\n",
    "\n",
    "# main\n",
    "X_train, X_validation, y_train, y_validation = loadDataSet()\n",
    "alpha = 0.1\n",
    "maxCycles = 500\n",
    "losss_train = gradDescent(alpha,maxCycles,X_train,y_train)\n",
    "losss_validation = gradDescent(alpha,maxCycles,X_validation,y_validation)\n",
    "plotLossPerTime(maxCycles,losss_train,losss_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(379, 13) (379,)\n",
      "loss = 306.535431\n",
      "loss = 108.215354\n",
      "loss = 54.247134\n",
      "loss = 38.640906\n",
      "loss = 33.400624\n",
      "loss = 31.084140\n",
      "loss = 29.680009\n",
      "loss = 28.621503\n",
      "loss = 27.733082\n",
      "loss = 26.949868\n",
      "loss = 26.241511\n",
      "loss = 25.590542\n",
      "loss = 24.985538\n",
      "loss = 24.418535\n",
      "loss = 23.883787\n",
      "loss = 23.377062\n",
      "loss = 22.895175\n",
      "loss = 22.435685\n",
      "loss = 21.996674\n",
      "loss = 21.576601\n",
      "loss = 21.174199\n",
      "loss = 20.788395\n",
      "loss = 20.418264\n",
      "loss = 20.062993\n",
      "loss = 19.721849\n",
      "loss = 19.394167\n",
      "loss = 19.079333\n",
      "loss = 18.776779\n",
      "loss = 18.485969\n",
      "loss = 18.206401\n",
      "loss = 17.937598\n",
      "loss = 17.679109\n",
      "loss = 17.430505\n",
      "loss = 17.191375\n",
      "loss = 16.961330\n",
      "loss = 16.739995\n",
      "loss = 16.527015\n",
      "loss = 16.322049\n",
      "loss = 16.124769\n",
      "loss = 15.934864\n",
      "loss = 15.752035\n",
      "loss = 15.575994\n",
      "loss = 15.406469\n",
      "loss = 15.243197\n",
      "loss = 15.085925\n",
      "loss = 14.934414\n",
      "loss = 14.788431\n",
      "loss = 14.647756\n",
      "loss = 14.512178\n",
      "loss = 14.381492\n",
      "loss = 14.255504\n",
      "loss = 14.134028\n",
      "loss = 14.016885\n",
      "loss = 13.903904\n",
      "loss = 13.794919\n",
      "loss = 13.689775\n",
      "loss = 13.588319\n",
      "loss = 13.490408\n",
      "loss = 13.395902\n",
      "loss = 13.304669\n",
      "loss = 13.216580\n",
      "loss = 13.131514\n",
      "loss = 13.049353\n",
      "loss = 12.969985\n",
      "loss = 12.893303\n",
      "loss = 12.819201\n",
      "loss = 12.747582\n",
      "loss = 12.678350\n",
      "loss = 12.611414\n",
      "loss = 12.546686\n",
      "loss = 12.484082\n",
      "loss = 12.423521\n",
      "loss = 12.364927\n",
      "loss = 12.308224\n",
      "loss = 12.253342\n",
      "loss = 12.200213\n",
      "loss = 12.148769\n",
      "loss = 12.098950\n",
      "loss = 12.050694\n",
      "loss = 12.003942\n",
      "loss = 11.958640\n",
      "loss = 11.914734\n",
      "loss = 11.872173\n",
      "loss = 11.830907\n",
      "loss = 11.790888\n",
      "loss = 11.752072\n",
      "loss = 11.714415\n",
      "loss = 11.677875\n",
      "loss = 11.642411\n",
      "loss = 11.607984\n",
      "loss = 11.574558\n",
      "loss = 11.542097\n",
      "loss = 11.510567\n",
      "loss = 11.479934\n",
      "loss = 11.450167\n",
      "loss = 11.421235\n",
      "loss = 11.393110\n",
      "loss = 11.365762\n",
      "loss = 11.339165\n",
      "loss = 11.313293\n",
      "loss = 11.288121\n",
      "loss = 11.263625\n",
      "loss = 11.239782\n",
      "loss = 11.216568\n",
      "loss = 11.193964\n",
      "loss = 11.171949\n",
      "loss = 11.150502\n",
      "loss = 11.129605\n",
      "loss = 11.109240\n",
      "loss = 11.089388\n",
      "loss = 11.070032\n",
      "loss = 11.051158\n",
      "loss = 11.032748\n",
      "loss = 11.014787\n",
      "loss = 10.997262\n",
      "loss = 10.980158\n",
      "loss = 10.963461\n",
      "loss = 10.947159\n",
      "loss = 10.931239\n",
      "loss = 10.915689\n",
      "loss = 10.900497\n",
      "loss = 10.885652\n",
      "loss = 10.871144\n",
      "loss = 10.856961\n",
      "loss = 10.843095\n",
      "loss = 10.829535\n",
      "loss = 10.816272\n",
      "loss = 10.803297\n",
      "loss = 10.790601\n",
      "loss = 10.778176\n",
      "loss = 10.766015\n",
      "loss = 10.754108\n",
      "loss = 10.742449\n",
      "loss = 10.731031\n",
      "loss = 10.719846\n",
      "loss = 10.708888\n",
      "loss = 10.698150\n",
      "loss = 10.687627\n",
      "loss = 10.677311\n",
      "loss = 10.667198\n",
      "loss = 10.657281\n",
      "loss = 10.647555\n",
      "loss = 10.638014\n",
      "loss = 10.628655\n",
      "loss = 10.619471\n",
      "loss = 10.610458\n",
      "loss = 10.601612\n",
      "loss = 10.592928\n",
      "loss = 10.584401\n",
      "loss = 10.576028\n",
      "loss = 10.567805\n",
      "loss = 10.559727\n",
      "loss = 10.551791\n",
      "loss = 10.543993\n",
      "loss = 10.536330\n",
      "loss = 10.528799\n",
      "loss = 10.521395\n",
      "loss = 10.514117\n",
      "loss = 10.506960\n",
      "loss = 10.499922\n",
      "loss = 10.493000\n",
      "loss = 10.486191\n",
      "loss = 10.479492\n",
      "loss = 10.472901\n",
      "loss = 10.466415\n",
      "loss = 10.460032\n",
      "loss = 10.453749\n",
      "loss = 10.447564\n",
      "loss = 10.441475\n",
      "loss = 10.435480\n",
      "loss = 10.429576\n",
      "loss = 10.423761\n",
      "loss = 10.418034\n",
      "loss = 10.412392\n",
      "loss = 10.406833\n",
      "loss = 10.401356\n",
      "loss = 10.395959\n",
      "loss = 10.390641\n",
      "loss = 10.385399\n",
      "loss = 10.380231\n",
      "loss = 10.375137\n",
      "loss = 10.370115\n",
      "loss = 10.365163\n",
      "loss = 10.360280\n",
      "loss = 10.355465\n",
      "loss = 10.350715\n",
      "loss = 10.346030\n",
      "loss = 10.341409\n",
      "loss = 10.336849\n",
      "loss = 10.332351\n",
      "loss = 10.327912\n",
      "loss = 10.323531\n",
      "loss = 10.319208\n",
      "loss = 10.314942\n",
      "loss = 10.310730\n",
      "loss = 10.306573\n",
      "loss = 10.302469\n",
      "loss = 10.298417\n",
      "loss = 10.294416\n",
      "loss = 10.290466\n",
      "loss = 10.286564\n",
      "loss = 10.282711\n",
      "loss = 10.278906\n",
      "loss = 10.275147\n",
      "loss = 10.271434\n",
      "loss = 10.267766\n",
      "loss = 10.264143\n",
      "loss = 10.260562\n",
      "loss = 10.257025\n",
      "loss = 10.253529\n",
      "loss = 10.250075\n",
      "loss = 10.246661\n",
      "loss = 10.243286\n",
      "loss = 10.239951\n",
      "loss = 10.236655\n",
      "loss = 10.233396\n",
      "loss = 10.230175\n",
      "loss = 10.226990\n",
      "loss = 10.223841\n",
      "loss = 10.220728\n",
      "loss = 10.217649\n",
      "loss = 10.214605\n",
      "loss = 10.211595\n",
      "loss = 10.208617\n",
      "loss = 10.205673\n",
      "loss = 10.202760\n",
      "loss = 10.199879\n",
      "loss = 10.197030\n",
      "loss = 10.194211\n",
      "loss = 10.191422\n",
      "loss = 10.188663\n",
      "loss = 10.185933\n",
      "loss = 10.183232\n",
      "loss = 10.180560\n",
      "loss = 10.177915\n",
      "loss = 10.175298\n",
      "loss = 10.172709\n",
      "loss = 10.170146\n",
      "loss = 10.167609\n",
      "loss = 10.165099\n",
      "loss = 10.162614\n",
      "loss = 10.160154\n",
      "loss = 10.157719\n",
      "loss = 10.155309\n",
      "loss = 10.152923\n",
      "loss = 10.150560\n",
      "loss = 10.148222\n",
      "loss = 10.145906\n",
      "loss = 10.143614\n",
      "loss = 10.141343\n",
      "loss = 10.139095\n",
      "loss = 10.136869\n",
      "loss = 10.134665\n",
      "loss = 10.132482\n",
      "loss = 10.130320\n",
      "loss = 10.128179\n",
      "loss = 10.126058\n",
      "loss = 10.123958\n",
      "loss = 10.121877\n",
      "loss = 10.119816\n",
      "loss = 10.117775\n",
      "loss = 10.115753\n",
      "loss = 10.113749\n",
      "loss = 10.111765\n",
      "loss = 10.109798\n",
      "loss = 10.107850\n",
      "loss = 10.105920\n",
      "loss = 10.104008\n",
      "loss = 10.102113\n",
      "loss = 10.100236\n",
      "loss = 10.098375\n",
      "loss = 10.096532\n",
      "loss = 10.094705\n",
      "loss = 10.092894\n",
      "loss = 10.091100\n",
      "loss = 10.089322\n",
      "loss = 10.087560\n",
      "loss = 10.085813\n",
      "loss = 10.084082\n",
      "loss = 10.082366\n",
      "loss = 10.080665\n",
      "loss = 10.078979\n",
      "loss = 10.077308\n",
      "loss = 10.075651\n",
      "loss = 10.074009\n",
      "loss = 10.072381\n",
      "loss = 10.070767\n",
      "loss = 10.069167\n",
      "loss = 10.067581\n",
      "loss = 10.066008\n",
      "loss = 10.064448\n",
      "loss = 10.062902\n",
      "loss = 10.061369\n",
      "loss = 10.059849\n",
      "loss = 10.058342\n",
      "loss = 10.056847\n",
      "loss = 10.055365\n",
      "loss = 10.053896\n",
      "loss = 10.052438\n",
      "loss = 10.050993\n",
      "loss = 10.049559\n",
      "loss = 10.048138\n",
      "loss = 10.046728\n",
      "loss = 10.045329\n",
      "loss = 10.043943\n",
      "loss = 10.042567\n",
      "loss = 10.041203\n",
      "loss = 10.039849\n",
      "loss = 10.038507\n",
      "loss = 10.037175\n",
      "loss = 10.035855\n",
      "loss = 10.034544\n",
      "loss = 10.033245\n",
      "loss = 10.031955\n",
      "loss = 10.030676\n",
      "loss = 10.029408\n",
      "loss = 10.028149\n",
      "loss = 10.026900\n",
      "loss = 10.025661\n",
      "loss = 10.024431\n",
      "loss = 10.023212\n",
      "loss = 10.022002\n",
      "loss = 10.020801\n",
      "loss = 10.019610\n",
      "loss = 10.018428\n",
      "loss = 10.017255\n",
      "loss = 10.016091\n",
      "loss = 10.014936\n",
      "loss = 10.013790\n",
      "loss = 10.012653\n",
      "loss = 10.011524\n",
      "loss = 10.010404\n",
      "loss = 10.009293\n",
      "loss = 10.008190\n",
      "loss = 10.007095\n",
      "loss = 10.006009\n",
      "loss = 10.004931\n",
      "loss = 10.003861\n",
      "loss = 10.002799\n",
      "loss = 10.001745\n",
      "loss = 10.000699\n",
      "loss = 9.999661\n",
      "loss = 9.998630\n",
      "loss = 9.997607\n",
      "loss = 9.996592\n",
      "loss = 9.995584\n",
      "loss = 9.994583\n",
      "loss = 9.993590\n",
      "loss = 9.992605\n",
      "loss = 9.991626\n",
      "loss = 9.990655\n",
      "loss = 9.989690\n",
      "loss = 9.988733\n",
      "loss = 9.987783\n",
      "loss = 9.986839\n",
      "loss = 9.985903\n",
      "loss = 9.984973\n",
      "loss = 9.984050\n",
      "loss = 9.983133\n",
      "loss = 9.982223\n",
      "loss = 9.981320\n",
      "loss = 9.980423\n",
      "loss = 9.979532\n",
      "loss = 9.978648\n",
      "loss = 9.977770\n",
      "loss = 9.976898\n",
      "loss = 9.976033\n",
      "loss = 9.975173\n",
      "loss = 9.974320\n",
      "loss = 9.973473\n",
      "loss = 9.972631\n",
      "loss = 9.971796\n",
      "loss = 9.970966\n",
      "loss = 9.970142\n",
      "loss = 9.969324\n",
      "loss = 9.968511\n",
      "loss = 9.967705\n",
      "loss = 9.966903\n",
      "loss = 9.966108\n",
      "loss = 9.965318\n",
      "loss = 9.964533\n",
      "loss = 9.963754\n",
      "loss = 9.962980\n",
      "loss = 9.962211\n",
      "loss = 9.961447\n",
      "loss = 9.960689\n",
      "loss = 9.959936\n",
      "loss = 9.959188\n",
      "loss = 9.958446\n",
      "loss = 9.957708\n",
      "loss = 9.956975\n",
      "loss = 9.956247\n",
      "loss = 9.955524\n",
      "loss = 9.954806\n",
      "loss = 9.954093\n",
      "loss = 9.953385\n",
      "loss = 9.952681\n",
      "loss = 9.951982\n",
      "loss = 9.951288\n",
      "loss = 9.950598\n",
      "loss = 9.949913\n",
      "loss = 9.949233\n",
      "loss = 9.948557\n",
      "loss = 9.947885\n",
      "loss = 9.947218\n",
      "loss = 9.946555\n",
      "loss = 9.945897\n",
      "loss = 9.945243\n",
      "loss = 9.944594\n",
      "loss = 9.943948\n",
      "loss = 9.943307\n",
      "loss = 9.942670\n",
      "loss = 9.942037\n",
      "loss = 9.941409\n",
      "loss = 9.940784\n",
      "loss = 9.940164\n",
      "loss = 9.939547\n",
      "loss = 9.938935\n",
      "loss = 9.938326\n",
      "loss = 9.937722\n",
      "loss = 9.937121\n",
      "loss = 9.936524\n",
      "loss = 9.935931\n",
      "loss = 9.935342\n",
      "loss = 9.934757\n",
      "loss = 9.934175\n",
      "loss = 9.933598\n",
      "loss = 9.933024\n",
      "loss = 9.932453\n",
      "loss = 9.931886\n",
      "loss = 9.931323\n",
      "loss = 9.930763\n",
      "loss = 9.930207\n",
      "loss = 9.929655\n",
      "loss = 9.929106\n",
      "loss = 9.928560\n",
      "loss = 9.928018\n",
      "loss = 9.927479\n",
      "loss = 9.926944\n",
      "loss = 9.926412\n",
      "loss = 9.925883\n",
      "loss = 9.925358\n",
      "loss = 9.924836\n",
      "loss = 9.924317\n",
      "loss = 9.923802\n",
      "loss = 9.923289\n",
      "loss = 9.922780\n",
      "loss = 9.922274\n",
      "loss = 9.921772\n",
      "loss = 9.921272\n",
      "loss = 9.920776\n",
      "loss = 9.920282\n",
      "loss = 9.919792\n",
      "loss = 9.919304\n",
      "loss = 9.918820\n",
      "loss = 9.918338\n",
      "loss = 9.917860\n",
      "loss = 9.917385\n",
      "loss = 9.916912\n",
      "loss = 9.916442\n",
      "loss = 9.915976\n",
      "loss = 9.915512\n",
      "loss = 9.915051\n",
      "loss = 9.914592\n",
      "loss = 9.914137\n",
      "loss = 9.913684\n",
      "loss = 9.913234\n",
      "loss = 9.912787\n",
      "loss = 9.912342\n",
      "loss = 9.911901\n",
      "loss = 9.911461\n",
      "loss = 9.911025\n",
      "loss = 9.910591\n",
      "loss = 9.910160\n",
      "loss = 9.909731\n",
      "loss = 9.909305\n",
      "loss = 9.908882\n",
      "loss = 9.908461\n",
      "loss = 9.908042\n",
      "loss = 9.907626\n",
      "loss = 9.907213\n",
      "loss = 9.906802\n",
      "loss = 9.906393\n",
      "loss = 9.905987\n",
      "loss = 9.905584\n",
      "loss = 9.905183\n",
      "loss = 9.904784\n",
      "loss = 9.904387\n",
      "loss = 9.903993\n",
      "loss = 9.903601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 9.903212\n",
      "loss = 9.902825\n",
      "loss = 9.902440\n",
      "loss = 9.902057\n",
      "loss = 9.901677\n",
      "loss = 9.901299\n",
      "loss = 9.900923\n",
      "loss = 9.900549\n",
      "loss = 9.900178\n",
      "loss = 9.899808\n",
      "[-9.38501146  1.73035793 -4.65678217 -1.86657919  2.32528401 -7.92909165\n",
      " -0.19495226  9.31935626 -3.59115311  1.11421123 -0.94681163  1.88395808\n",
      " -6.83012265]\n",
      "[ 9.16193818]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b1163e82b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def loadDataSet():\n",
    "\t# 读取数据\n",
    "\tX,y=load_svmlight_file(\"housing_scale.txt\")\n",
    "\t#将稀疏矩阵转化为完整特征矩阵\n",
    "\tX.todense()\n",
    "\t# 将数据集切分为训练集和验证集\n",
    "\tX_train, X_validation, y_train, y_validation = train_test_split(X, y, random_state=0)\n",
    "\tprint(X_train.shape,y_train.shape)\n",
    "\treturn X_train, X_validation, y_train, y_validation\n",
    "\n",
    "def gradDescent(alpha,maxCycles,X_data,y_data):\n",
    "\tnum = y_data.shape[0]    #样本数量\n",
    "\t# 线性模型参数正态分布初始化\n",
    "\tw = np.random.normal(size=(X_data.shape[1]))\n",
    "\tb = np.random.normal(size=1)\n",
    "\tlosss = []\n",
    "\n",
    "\t#迭代次maxCycles次\n",
    "\tfor n in range(maxCycles):\n",
    "\t\tgrad_w = np.zeros(X_data.shape[1])\n",
    "\t\tgrad_b = np.zeros(1)\n",
    "\t\tloss = 0\n",
    "\t\tfor i in range(num):\n",
    "\t\t\ty = np.dot( X_data[i].data, w ) + b\n",
    "\t\t\tloss += np.power((y - y_data[i]),2) / ( 2 * num)\n",
    "\t\t\tgrad_w += ( y - y_data[i] ) * X_data[i].data / num\n",
    "\t\t\tgrad_b += ( y - y_data[i] ) / num\n",
    "\t\t#更新模型参数\n",
    "\t\tw -= alpha * grad_w \n",
    "\t\tb -= alpha * grad_b\n",
    "\t\tlosss.append(loss)\n",
    "\t\tprint(\"loss = %f\" % loss)\n",
    "\tprint(w)\n",
    "\tprint(b)\n",
    "\treturn losss\n",
    "\n",
    "def plotLossPerTime(n,losss_train,losss_validation):\n",
    "\tplt.xlabel('iteration times')\n",
    "\tplt.ylabel('loss of train or validation')\n",
    "\tplt.title('linear regression & gradient decrease')\n",
    "\tn_cycles = range(1,n+1)\n",
    "\tplt.plot(n_cycles, losss_train, label = \"Loss of Train\", color='blue', linewidth=3)\n",
    "\tplt.plot(n_cycles, losss_validation, label = \"Loss of Validation\", color='red', linewidth=3)\n",
    "\tplt.legend(loc=0)\n",
    "\tplt.grid()\n",
    "\tplt.show()\n",
    "\n",
    "# main\n",
    "X_train, X_validation, y_train, y_validation = loadDataSet()\n",
    "alpha = 0.1\n",
    "maxCycles = 500\n",
    "losss_train = gradDescent(alpha,maxCycles,X_train,y_train)\n",
    "#losss_validation = gradDescent(alpha,maxCycles,X_validation,y_validation)\n",
    "plotLossPerTime(maxCycles,losss_train,losss_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
